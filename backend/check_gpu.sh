#!/bin/bash

echo "========================================="
echo "   ğŸ® GPU ç’°å¢ƒæª¢æ¸¬å·¥å…·"
echo "========================================="

# é€²å…¥è™›æ“¬ç’°å¢ƒ
cd /home/k66/suiguard/backend
source venv/bin/activate

echo ""
echo "ğŸ“Š ç³»çµ±è³‡è¨Š:"
echo "-----------------------------------"

# 1. GPU ç¡¬é«”è³‡è¨Š
echo "ğŸ” GPU ç¡¬é«”:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader
else
    echo "  âŒ nvidia-smi æœªå®‰è£æˆ–ç„¡ GPU"
fi

echo ""
echo "ğŸ” CUDA ç‰ˆæœ¬:"
if command -v nvcc &> /dev/null; then
    nvcc --version | grep "release" | awk '{print "  " $5 $6}'
else
    if command -v nvidia-smi &> /dev/null; then
        echo "  âœ… CUDA $(nvidia-smi | grep "CUDA Version" | awk '{print $9}')"
    else
        echo "  âŒ CUDA æœªå®‰è£"
    fi
fi

echo ""
echo "ğŸ” PyTorch ç’°å¢ƒ:"
python << 'PYEOF'
import torch
import sys

print(f"  â€¢ PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"  â€¢ Python ç‰ˆæœ¬: {sys.version.split()[0]}")

if torch.cuda.is_available():
    print(f"  â€¢ CUDA å¯ç”¨: âœ… YES")
    print(f"  â€¢ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  â€¢ GPU æ•¸é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"  â€¢ GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"    - VRAM: {props.total_memory / 1024**3:.1f} GB")
        print(f"    - è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print(f"  â€¢ CUDA å¯ç”¨: âŒ NO (å°‡ä½¿ç”¨ CPU)")
PYEOF

echo ""
echo "========================================="
echo "ğŸ“Š ç•¶å‰ GPU ä½¿ç”¨æƒ…æ³:"
echo "========================================="
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv
fi

echo ""
echo "========================================="
echo "ğŸ§ª å¿«é€Ÿæ¸¬è©¦ GPU é‹ç®—:"
echo "========================================="
python << 'PYEOF'
import torch
import time

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"âœ… ä½¿ç”¨è¨­å‚™: {device}")
    
    # ç°¡å–®çš„å¼µé‡é‹ç®—æ¸¬è©¦
    print("ğŸ”„ åŸ·è¡Œ GPU é‹ç®—æ¸¬è©¦...")
    x = torch.randn(1000, 1000).to(device)
    y = torch.randn(1000, 1000).to(device)
    
    start = time.time()
    z = torch.matmul(x, y)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    print(f"âœ… çŸ©é™£ä¹˜æ³•æ¸¬è©¦å®Œæˆ ({elapsed*1000:.2f} ms)")
    print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated()/1024**2:.1f} MB")
else:
    device = torch.device("cpu")
    print(f"âš ï¸ ä½¿ç”¨è¨­å‚™: {device}")
    print("ğŸ’¡ ç„¡ GPU å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
PYEOF

echo ""
echo "========================================="
echo "âœ… æª¢æ¸¬å®Œæˆ"
echo "========================================="
