# ML Training Service - ä½¿ç”¨èªªæ˜

## ğŸ“‹ æ¦‚è¿°

ML Training Service æ˜¯ SuiGuard çš„æ©Ÿå™¨å­¸ç¿’è¨“ç·´æœå‹™ï¼Œç”¨æ–¼è¨“ç·´å’Œæ¸¬è©¦ Sui æ™ºèƒ½åˆç´„æ¼æ´æª¢æ¸¬æ¨¡å‹ã€‚

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å‘½ä»¤è¡Œä½¿ç”¨

#### è¨“ç·´æ¨¡å‹
```bash
# åŸºæœ¬è¨“ç·´ï¼ˆä½¿ç”¨é»˜èªåƒæ•¸ï¼‰
python ml_cli.py --train

# è‡ªå®šç¾©è¨“ç·´åƒæ•¸
python ml_cli.py --train \
  --epochs 3 \
  --batch-size 2 \
  --learning-rate 3e-4 \
  --lora-r 8 \
  --lora-alpha 16

# ä½¿ç”¨ä¸åŒçš„åŸºç¤æ¨¡å‹
python ml_cli.py --train \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --dataset contract_bug_dataset.jsonl \
  --output ./output_lora_bug
```

#### æ¸¬è©¦æ¨¡å‹
```bash
# æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹
python ml_cli.py --test

# æ¸¬è©¦æŒ‡å®šè·¯å¾‘çš„æ¨¡å‹
python ml_cli.py --test --model-path ./output_lora_bug
```

#### äº¤å‰é©—è­‰
```bash
# 3-fold äº¤å‰é©—è­‰
python ml_cli.py --cross-validate --folds 3

# 5-fold äº¤å‰é©—è­‰
python ml_cli.py --cross-validate --folds 5
```

### 2. Python ä»£ç¢¼ä½¿ç”¨

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•¸

```python
from services import train_vulnerability_detection_model, test_vulnerability_detection_model

# è¨“ç·´æ¨¡å‹
train_stats = train_vulnerability_detection_model(
    base_model="mistralai/Mistral-7B-Instruct-v0.2",
    dataset_path="contract_bug_dataset.jsonl",
    output_dir="./output_lora_bug",
    num_epochs=3
)

# æ¸¬è©¦æ¨¡å‹
test_results = test_vulnerability_detection_model(
    model_path="./output_lora_bug",
    dataset_path="contract_bug_dataset.jsonl"
)

print(f"æº–ç¢ºç‡: {test_results['summary']['accuracy']:.1f}%")
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨æœå‹™é¡

```python
from services.ml_training_service import MLTrainingService

# å‰µå»ºæœå‹™å¯¦ä¾‹
service = MLTrainingService(
    base_model="mistralai/Mistral-7B-Instruct-v0.2",
    dataset_path="contract_bug_dataset.jsonl",
    output_dir="./output_lora_bug"
)

# è¨“ç·´æ¨¡å‹
train_results = service.train_model(
    num_epochs=3,
    batch_size=2,
    learning_rate=3e-4,
    lora_r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    use_8bit=True
)

# æ¸¬è©¦æ¨¡å‹
test_results = service.test_model()

# äº¤å‰é©—è­‰
cv_results = service.cross_validate(n_folds=3)
```

## ğŸ“Š æ•¸æ“šé›†æ ¼å¼

æ•¸æ“šé›†æ‡‰ç‚º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€å€‹ JSON å°è±¡ï¼‰ï¼š

```jsonl
{"instruction": "æ‰¾å‡ºSui smart contractçš„æƒ¡æ„æ¼æ´ï¼Œä¸¦åˆ¤æ–·æ˜¯å“ªç¨®æ¼æ´é¡å‹(5ç¨®æ¼æ´(Resource Leak, Arithmetic Overflow, Unchecked Return, Cross-Module Pollution, Capability Leak)æˆ–æœªç™¼ç¾æ˜é¡¯æ¼æ´)ï¼Œç”¨ç¹é«”ä¸­æ–‡å›ç­”", "input": "public fun withdraw(coin: Coin, amount: u64, ctx: &TxContext) { assert!(coin.balance >= amount, 0); coin.balance = coin.balance - amount; }", "output": "æ¼æ´é¡å‹ï¼šArithmetic Overflowï¼Œè¡Œæ•¸ï¼šç¬¬3è¡Œæ¸›æ³•é‹ç®—æœªåšå¥½ä¸‹æº¢æª¢æŸ¥"}
```

### æ”¯æŒçš„æ¼æ´é¡å‹

1. **Resource Leak** - è³‡æºæ´©æ¼
2. **Arithmetic Overflow** - ç®—è¡“æº¢ä½
3. **Unchecked Return** - æœªæª¢æŸ¥è¿”å›å€¼
4. **Cross-Module Pollution** - è·¨æ¨¡çµ„æ±¡æŸ“
5. **Capability Leak** - æ¬Šé™æ´©æ¼
6. **æœªç™¼ç¾æ˜é¡¯æ¼æ´** - å®‰å…¨ä»£ç¢¼

## ğŸ¯ è¨“ç·´åƒæ•¸èªªæ˜

| åƒæ•¸ | é»˜èªå€¼ | èªªæ˜ |
|------|--------|------|
| `--model` | Mistral-7B-Instruct-v0.2 | åŸºç¤æ¨¡å‹åç¨± |
| `--dataset` | contract_bug_dataset.jsonl | è¨“ç·´æ•¸æ“šé›†è·¯å¾‘ |
| `--output` | ./output_lora_bug | æ¨¡å‹è¼¸å‡ºç›®éŒ„ |
| `--epochs` | 3 | è¨“ç·´è¼ªæ•¸ |
| `--batch-size` | 2 | æ‰¹æ¬¡å¤§å° |
| `--learning-rate` | 3e-4 | å­¸ç¿’ç‡ |
| `--lora-r` | 8 | LoRA ç§© (rank) |
| `--lora-alpha` | 16 | LoRA alpha åƒæ•¸ |
| `--lora-dropout` | 0.05 | LoRA dropout |
| `--no-8bit` | False | ä¸ä½¿ç”¨ 8-bit é‡åŒ– |

## ğŸ“ˆ æ¸¬è©¦çµæœç¤ºä¾‹

```
================================================================================
  ğŸ“Š æ¸¬è©¦çµæœç¸½çµ
================================================================================
æ¸¬è©¦æ¨£æœ¬æ•¸: 66
âœ… æ­£ç¢ºæ•¸é‡: 61
âŒ éŒ¯èª¤æ•¸é‡: 5
æº–ç¢ºç‡: 92.4%
å¹³å‡ç›¸ä¼¼åº¦: 81.1%
å¹³å‡æ¨ç†æ™‚é–“: 29.67 ç§’

================================================================================
  ğŸ“‹ æŒ‰æ¼æ´é¡å‹åˆ†æ
================================================================================
Arithmetic Overflow: 11/11 (100.0%)
Capability Leak: 11/11 (100.0%)
Cross-Module Pollution: 11/11 (100.0%)
Resource Leak: 9/11 (81.8%)
Unchecked Return: 11/11 (100.0%)
æœªç™¼ç¾æ˜é¡¯æ¼æ´: 8/11 (72.7%)
================================================================================
```

## ğŸ“ è¼¸å‡ºæ–‡ä»¶

è¨“ç·´å’Œæ¸¬è©¦æœƒç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
output_lora_bug/
â”œâ”€â”€ adapter_config.json          # LoRA é…ç½®
â”œâ”€â”€ adapter_model.safetensors    # LoRA æ¬Šé‡
â”œâ”€â”€ tokenizer.json               # åˆ†è©å™¨
â”œâ”€â”€ tokenizer_config.json        # åˆ†è©å™¨é…ç½®
â”œâ”€â”€ training_stats.json          # è¨“ç·´çµ±è¨ˆ
â”œâ”€â”€ test_results.json            # æ¸¬è©¦çµæœ
â””â”€â”€ cross_validation_results.json # äº¤å‰é©—è­‰çµæœï¼ˆå¦‚æœ‰ï¼‰
```

### training_stats.json æ ¼å¼
```json
{
  "train_time": 3600.50,
  "train_samples": 66,
  "num_epochs": 3,
  "final_loss": 0.7435,
  "model_path": "./output_lora_bug",
  "timestamp": "2025-10-13T15:00:00"
}
```

### test_results.json æ ¼å¼
```json
{
  "test_time": "2025-10-13T16:00:00",
  "model_path": "./output_lora_bug",
  "summary": {
    "total_samples": 66,
    "correct_count": 61,
    "accuracy": 92.4,
    "avg_similarity": 81.1,
    "avg_time": 29.67,
    "total_time": 1958.46
  },
  "vulnerability_stats": {
    "Arithmetic Overflow": {"total": 11, "correct": 11},
    "Capability Leak": {"total": 11, "correct": 11},
    ...
  },
  "results": [...]
}
```

## ğŸ”§ é€²éšä½¿ç”¨

### 1. æ•´åˆåˆ° FastAPI å¾Œç«¯

```python
from fastapi import FastAPI, BackgroundTasks
from services.ml_training_service import MLTrainingService

app = FastAPI()
ml_service = MLTrainingService()

@app.post("/api/ml/train")
async def train_model(background_tasks: BackgroundTasks):
    """å¾Œå°è¨“ç·´æ¨¡å‹"""
    background_tasks.add_task(ml_service.train_model)
    return {"status": "è¨“ç·´å·²é–‹å§‹"}

@app.get("/api/ml/test")
async def test_model():
    """æ¸¬è©¦æ¨¡å‹"""
    results = ml_service.test_model()
    return results['summary']
```

### 2. ä½¿ç”¨è‡ªå®šç¾©æ•¸æ“šé›†

```python
# è¼‰å…¥è‡ªå®šç¾©æ•¸æ“š
custom_samples = [
    {
        "instruction": "...",
        "input": "...",
        "output": "..."
    },
    ...
]

# æ¸¬è©¦è‡ªå®šç¾©æ¨£æœ¬
results = service.test_model(test_samples=custom_samples)
```

### 3. æ¨¡å‹æ¨ç†

```python
from services.ml_training_service import MLTrainingService

service = MLTrainingService(output_dir="./output_lora_bug")

# å–®å€‹æ¨ç†
output, time = service._inference(
    model=service.model,
    tokenizer=service.tokenizer,
    instruction="æ‰¾å‡ºæ¼æ´...",
    input_text="public fun withdraw(...)"
)
```

## ğŸ› æ•…éšœæ’é™¤

### å•é¡Œï¼šCUDA è¨˜æ†¶é«”ä¸è¶³
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- é™ä½ `batch_size`
- ä½¿ç”¨ 8-bit é‡åŒ–ï¼šä¸åŠ  `--no-8bit` æ¨™èªŒ
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ TinyLlama-1.1Bï¼‰

### å•é¡Œï¼šè¨“ç·´é€Ÿåº¦æ…¢
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- æ¸›å°‘ `epochs` æ•¸é‡
- å¢å¤§ `batch_size`ï¼ˆå¦‚æœè¨˜æ†¶é«”å…è¨±ï¼‰
- ä½¿ç”¨æ›´å°çš„ `lora_r` å€¼

### å•é¡Œï¼šæº–ç¢ºç‡ä½
**è§£æ±ºæ–¹æ¡ˆ**ï¼š
- å¢åŠ è¨“ç·´æ•¸æ“šé‡
- å¢åŠ  `epochs` æ•¸é‡
- èª¿æ•´ `learning_rate`
- ä½¿ç”¨æ›´å¼·å¤§çš„åŸºç¤æ¨¡å‹

## ğŸ“š ç›¸é—œæ–‡ä»¶

- `services/ml_training_service.py` - æ ¸å¿ƒè¨“ç·´æœå‹™
- `ml_cli.py` - å‘½ä»¤è¡Œç•Œé¢
- `services/risk_engine.py` - é¢¨éšªè©•ä¼°å¼•æ“ï¼ˆä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼‰
- `contract_bug_dataset.jsonl` - è¨“ç·´æ•¸æ“šé›†

## ğŸ“ æœ€ä½³å¯¦è¸

1. **æ•¸æ“šæº–å‚™**ï¼š
   - ç¢ºä¿æ•¸æ“šé›†å¹³è¡¡ï¼ˆæ¯ç¨®é¡å‹æ¨£æœ¬æ•¸é‡ç›¸è¿‘ï¼‰
   - ä½¿ç”¨é«˜è³ªé‡çš„æ¨™è¨»æ•¸æ“š
   - å®šæœŸæ›´æ–°æ•¸æ“šé›†

2. **è¨“ç·´ç­–ç•¥**ï¼š
   - å…ˆç”¨å°æ•¸æ“šé›†å¿«é€Ÿé©—è­‰
   - ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹ç©©å®šæ€§
   - ä¿å­˜è¨“ç·´æ—¥èªŒä»¥ä¾¿è¿½è¹¤

3. **éƒ¨ç½²å»ºè­°**ï¼š
   - åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ç¶“éå……åˆ†æ¸¬è©¦çš„æ¨¡å‹
   - å®šæœŸç›£æ§æ¨¡å‹æ€§èƒ½
   - å»ºç«‹æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

## ğŸ“ æ”¯æŒ

å¦‚æœ‰å•é¡Œï¼Œè«‹è¯ç¹«é–‹ç™¼åœ˜éšŠæˆ–æäº¤ Issueã€‚
