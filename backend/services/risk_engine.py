from typing import List, Dict, Optional
import re
from datetime import datetime
import json
import aiohttp
import os
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.metrics import f1_score
import torch

class RiskEngine:
    """é¢¨éšªè©•ä¼°å¼•æ“ - 
    è² è²¬åˆ†æåŸŸåã€æ¬Šé™å’Œæ™ºèƒ½åˆç´„åŒ…çš„é¢¨éšªç­‰ç´š
    æä¾›ç¶œåˆæ€§çš„å®‰å…¨é¢¨éšªè©•ä¼°å’Œå»ºè­°
    """
    
    def __init__(self):
        # LoRA å¾®èª¿æ¨¡å‹é…ç½®
        self.model_path = os.getenv("LORA_MODEL_PATH", "./lora_models")
        self.base_model_name = os.getenv("BASE_MODEL_NAME", "mistralai/Mistral-7B-v0.1")
        self.dataset_path = os.getenv("DATASET_PATH", "ml/contract_bug_dataset.jsonl")
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è©å™¨
        self.model = None
        self.tokenizer = None
        self._initialize_model()
        
        # æ¼æ´åˆ†é¡æ˜ å°„åˆ°é¢¨éšªåˆ†æ•¸å€é–“ (100åˆ†åˆ¶)
        self.vulnerability_score_ranges = {
            "access_control": (80, 100),    # å­˜å–æ§åˆ¶æ¼æ´ - é«˜é¢¨éšª (80-100åˆ†)
            "logic_error": (50, 79),        # é‚è¼¯éŒ¯èª¤æ¼æ´ - ä¸­é¢¨éšª (50-79åˆ†)  
            "randomness_error": (20, 49),   # éš¨æ©Ÿæ•¸æ¼æ´ - ä½é¢¨éšª (20-49åˆ†)
            "safe": (0, 19)                 # å®‰å…¨ä»£ç¢¼ - ç„¡é¢¨éšª (0-19åˆ†)
        }
        
        # æ©Ÿç‡åˆ†å¸ƒé–¾å€¼é…ç½®
        self.confidence_thresholds = {
            "high_confidence": 0.8,      # é«˜ä¿¡å¿ƒåº¦é–¾å€¼
            "medium_confidence": 0.6,    # ä¸­ä¿¡å¿ƒåº¦é–¾å€¼
            "low_confidence": 0.4        # ä½ä¿¡å¿ƒåº¦é–¾å€¼
        }
        
        # æƒ¡æ„åŸŸåé—œéµå­—æ¸…å–®
        self.malicious_domains = {
            'phishing', 'fake', 'scam', 'steal', 'malicious', 'hack', 
            'fraud', 'theft', 'fishing', 'wallet-stealer', 'crypto-steal',
            'bitcoin-scam', 'eth-fake', 'sui-fake', 'defi-scam', 'nft-steal',
            'metamask-fake', 'phantom-fake', 'ledger-fake', 'trezor-fake'
        }
        
        # å¯ç–‘åŸŸåé—œéµå­—æ¸…å–®
        self.suspicious_domains = {
            'free', 'bonus', 'gift', 'earn', 'quick', 'fast', 'easy',
            'double', 'triple', 'profit', 'money', 'rich', 'millionaire',
            'lottery', 'winner', 'prize', 'reward', 'airdrop-free'
        }
        
        # é«˜é¢¨éšªæ¬Šé™æ¸…å–®
        self.high_risk_permissions = {
            'wallet:sign', 'wallet:transfer', 'wallet:approve_all',
            'wallet:delegate', 'wallet:admin'
        }
        
        # ä¸­ç­‰é¢¨éšªæ¬Šé™æ¸…å–®
        self.medium_risk_permissions = {
            'wallet:read_balance', 'wallet:read_history', 'wallet:connect'
        }
        
        # å·²çŸ¥å®‰å…¨çš„å®˜æ–¹åŸŸå
        self.trusted_domains = {
            'sui.io', 'mysten.io', 'suiwallet.com', 'ethoswallet.com',
            'martianwallet.xyz', 'github.com', 'chrome.google.com'
        }
        
        # å®˜æ–¹SuiåŒ…åœ°å€
        self.official_sui_packages = {
            "0x0000000000000000000000000000000000000000000000000000000000000001",  # Move stdlib
            "0x0000000000000000000000000000000000000000000000000000000000000002",  # Sui framework
            "0x0000000000000000000000000000000000000000000000000000000000000003"   # Sui system
        }
    
    def _initialize_model(self):
        """åˆå§‹åŒ– LoRA å¾®èª¿æ¨¡å‹"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è¼‰åŸºç¤æ¨¡å‹: {self.base_model_name}")
            
            # åŠ è¼‰ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            
            # åŠ è¼‰åŸºç¤æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            # é…ç½® LoRA
            lora_config = LoraConfig(
                r=8,
                lora_alpha=16,
                target_modules=["q_proj", "v_proj"],
                lora_dropout=0.05,
                bias="none",
                task_type="CAUSAL_LM"
            )
            
            # å¦‚æœå­˜åœ¨å·²è¨“ç·´çš„æ¨¡å‹ï¼Œç›´æ¥åŠ è¼‰ LoRA æ¨¡å‹
            if os.path.exists(self.model_path):
                print(f"âœ… åŠ è¼‰å¾®èª¿æ¬Šé‡: {self.model_path}")
                try:
                    self.model = PeftModel.from_pretrained(base_model, self.model_path)
                    print("âœ… LoRA å¾®èª¿æ¨¡å‹åŠ è¼‰æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ LoRA åŠ è¼‰å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤æ¨¡å‹: {e}")
                    self.model = get_peft_model(base_model, lora_config)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å¾®èª¿æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¤ LoRA é…ç½®: {self.model_path}")
                self.model = get_peft_model(base_model, lora_config)
            
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è¼‰å®Œæˆä¸¦è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            self.model = None
            self.tokenizer = None
    
    def load_dataset(self, jsonl_path: str) -> List[Dict]:
        """åŠ è¼‰è¨“ç·´æ•¸æ“šé›†"""
        lines = []
        try:
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    obj = json.loads(line)
                    lines.append({
                        "prompt": f"{obj['instruction']}\n{obj['input']}",
                        "output": obj['output']
                    })
            print(f"âœ… æˆåŠŸåŠ è¼‰ {len(lines)} ç­†è¨“ç·´æ•¸æ“š")
        except Exception as e:
            print(f"âŒ æ•¸æ“šé›†åŠ è¼‰å¤±æ•—: {e}")
        return lines
    
    def extract_label(self, output_text: str) -> str:
        """å¾è¼¸å‡ºæ–‡æœ¬æå–æ¼æ´æ¨™ç±¤"""
        output_lower = output_text.lower()
        
        if "æœªç™¼ç¾æ˜é¡¯æ¼æ´" in output_text or "æœªç™¼ç¾æ¼æ´" in output_text:
            return "safe"
        elif "é‡å…¥æ”»æ“Š" in output_text or "reentrancy" in output_lower:
            return "reentrancy"
        elif "æº¢ä½" in output_text or "overflow" in output_lower:
            return "overflow"
        elif "å­˜å–æ§åˆ¶" in output_text or "access_control" in output_lower:
            return "access_control"
        elif "é‚è¼¯éŒ¯èª¤" in output_text or "logic_error" in output_lower:
            return "logic_error"
        elif "éš¨æ©Ÿæ•¸" in output_text or "randomness" in output_lower:
            return "randomness_error"
        else:
            return "other_bug"
    
    def calculate_f1_score(self, dataset: List[Dict]) -> float:
        """è¨ˆç®—æ¨¡å‹çš„ F1 åˆ†æ•¸"""
        if not self.model or not self.tokenizer:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œç„¡æ³•è¨ˆç®— F1 åˆ†æ•¸")
            return 0.0
        
        y_true = []
        y_pred = []
        
        print(f"ğŸ“Š é–‹å§‹è©•ä¼° {len(dataset)} ç­†æ•¸æ“š...")
        
        for i, sample in enumerate(dataset):
            if i % 10 == 0:
                print(f"  è™•ç†é€²åº¦: {i}/{len(dataset)}")
            
            prompt = sample['prompt']
            label_true = self.extract_label(sample['output'])
            
            # ç”Ÿæˆé æ¸¬
            input_ids = self.tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).input_ids
            if torch.cuda.is_available():
                input_ids = input_ids.to('cuda')
            
            with torch.no_grad():
                output_ids = self.model.generate(
                    input_ids=input_ids,
                    max_new_tokens=128,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            label_pred = self.extract_label(output_text)
            
            y_true.append(label_true)
            y_pred.append(label_pred)
        
        # è¨ˆç®— F1 åˆ†æ•¸
        f1 = f1_score(y_true, y_pred, average='macro')
        print(f"âœ… F1 åˆ†æ•¸ï¼ˆmacroï¼‰: {f1:.4f}")
        return f1
    
    def analyze_domain_risk(self, domain: str) -> Dict:
        """åˆ†æåŸŸåé¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        domain_lower = domain.lower()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºä¿¡ä»»åŸŸå
        for trusted in self.trusted_domains:
            if trusted in domain_lower:
                return {
                    "risk_score": 0.0,
                    "reasons": [f"ä¿¡ä»»åŸŸå: {trusted}"]
                }
        
        # æª¢æŸ¥æƒ¡æ„é—œéµå­—
        for keyword in self.malicious_domains:
            if keyword in domain_lower:
                risk_score += 0.8
                reasons.append(f"é«˜é¢¨éšªåŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥å¯ç–‘é—œéµå­—  
        for keyword in self.suspicious_domains:
            if keyword in domain_lower:
                risk_score += 0.3
                reasons.append(f"å¯ç–‘åŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥åŸŸåé•·åº¦ç•°å¸¸
        if len(domain) > 30:
            risk_score += 0.2
            reasons.append("åŸŸåé•·åº¦ç•°å¸¸")
        
        # æª¢æŸ¥éå¤šé€£å­—ç¬¦
        if domain.count('-') > 2:
            risk_score += 0.3
            reasons.append("åŸŸååŒ…å«éå¤šé€£å­—ç¬¦")
        
        # æª¢æŸ¥æ•¸å­—å­—æ¯æ··åˆæ¨¡å¼
        if any(c.isdigit() for c in domain) and any(c.isalpha() for c in domain):
            digit_count = sum(c.isdigit() for c in domain)
            if digit_count > 3:
                risk_score += 0.2
                reasons.append("å¯ç–‘çš„æ•¸å­—å­—æ¯çµ„åˆ")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons
        }
    
    def analyze_permissions_risk(self, permissions: List[str]) -> Dict:
        """åˆ†ææ¬Šé™é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        high_risk_count = 0
        medium_risk_count = 0
        
        for permission in permissions:
            if permission in self.high_risk_permissions:
                risk_score += 0.4
                high_risk_count += 1
                reasons.append(f"é«˜é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
            elif permission in self.medium_risk_permissions:
                risk_score += 0.2
                medium_risk_count += 1
                reasons.append(f"ä¸­ç­‰é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
        
        # æ¬Šé™æ•¸é‡é¢¨éšªè©•ä¼°
        total_permissions = len(permissions)
        if total_permissions > 5:
            risk_score += 0.3
            reasons.append(f"è«‹æ±‚éå¤šæ¬Šé™: {total_permissions}å€‹")
        elif total_permissions > 3:
            risk_score += 0.1
            reasons.append(f"è«‹æ±‚è¼ƒå¤šæ¬Šé™: {total_permissions}å€‹")
        
        # é«˜é¢¨éšªæ¬Šé™çµ„åˆæª¢æŸ¥
        if 'wallet:sign' in permissions and 'wallet:transfer' in permissions:
            risk_score += 0.2
            reasons.append("å±éšªæ¬Šé™çµ„åˆ: ç°½å+è½‰å¸³")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "high_risk_permissions": high_risk_count,
            "medium_risk_permissions": medium_risk_count
        }
    
    def analyze_package_risk(self, package_analyses: List[Dict]) -> Dict:
        """åˆ†ææ™ºèƒ½åˆç´„åŒ…é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        analyzed_count = 0
        
        for analysis in package_analyses:
            if analysis.get('status') != 'success':
                continue
                
            analyzed_count += 1
            pkg_analysis = analysis.get('analysis', {})
            package_id = pkg_analysis.get('package_id', '')
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå®˜æ–¹SuiåŒ…
            if package_id in self.official_sui_packages:
                reasons.append("å®˜æ–¹Suiå¥—ä»¶ - å·²é©—è­‰å®‰å…¨")
                continue
            
            # åˆ†æå±éšªå‡½æ•¸
            dangerous_functions = pkg_analysis.get('dangerous_functions', [])
            if len(dangerous_functions) > 10:
                risk_score += 0.4
                reasons.append(f"æª¢æ¸¬åˆ°å¤§é‡å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
            elif len(dangerous_functions) > 5:
                risk_score += 0.2
                reasons.append(f"æª¢æ¸¬åˆ°å¤šå€‹å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "analyzed_packages": analyzed_count
        }
    
    def calculate_overall_risk(self, domain: str, permissions: List[str], package_analyses: List[Dict]) -> Dict:
        """ç¶œåˆé¢¨éšªè©•ä¼° - ä¸»è¦æ–¹æ³•"""
        
        # å„é …é¢¨éšªåˆ†æ
        domain_risk = self.analyze_domain_risk(domain)
        permission_risk = self.analyze_permissions_risk(permissions)
        package_risk = self.analyze_package_risk(package_analyses)
        
        # è¨ˆç®—åŠ æ¬Šé¢¨éšªåˆ†æ•¸
        # åŸŸåé¢¨éšªæ¬Šé‡æœ€é«˜ï¼Œå› ç‚ºæƒ¡æ„åŸŸåé€šå¸¸æ˜¯æœ€æ˜é¡¯çš„å±éšªä¿¡è™Ÿ
        domain_weight = 0.5
        permission_weight = 0.3
        package_weight = 0.2
        
        weighted_risk = (
            domain_risk['risk_score'] * domain_weight +
            permission_risk['risk_score'] * permission_weight +
            package_risk['risk_score'] * package_weight
        )
        
        # å¦‚æœä»»ä¸€é …ç›®é¢¨éšªæ¥µé«˜ï¼Œå‰‡ç¸½é¢¨éšªä¹Ÿæ‡‰è©²å¾ˆé«˜
        max_individual_risk = max(
            domain_risk['risk_score'],
            permission_risk['risk_score'],
            package_risk['risk_score']
        )
        
        # ä½¿ç”¨åŠ æ¬Šå¹³å‡å’Œæœ€é«˜å€‹åˆ¥é¢¨éšªçš„è¼ƒå¤§å€¼
        total_risk = max(weighted_risk, max_individual_risk * 0.8)
        
        # åˆä½µæ‰€æœ‰é¢¨éšªåŸå› 
        all_reasons = (
            domain_risk['reasons'] + 
            permission_risk['reasons'] + 
            package_risk['reasons']
        )
        
        # ç¢ºå®šé¢¨éšªç­‰ç´šå’Œå»ºè­°
        if total_risk >= 0.7:
            risk_level = "HIGH"
            recommendation = "æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª"
        elif total_risk >= 0.4:
            risk_level = "MEDIUM" 
            recommendation = "è­¦å‘Š - è«‹è¬¹æ…è™•ç†"
        else:
            risk_level = "LOW"
            recommendation = "æ‰¹å‡† - æª¢æ¸¬åˆ°ä½é¢¨éšª"
        
        return {
            "risk_level": risk_level,
            "confidence": round(total_risk, 2),
            "reasons": all_reasons,
            "recommendation": recommendation,
            "risk_breakdown": {
                "domain_risk": round(domain_risk['risk_score'], 2),
                "permission_risk": round(permission_risk['risk_score'], 2),
                "package_risk": round(package_risk['risk_score'], 2),
                "weighted_score": round(weighted_risk, 2),
                "final_score": round(total_risk, 2)
            },
            "details": {
                "analyzed_packages": package_risk['analyzed_packages'],
                "high_risk_permissions": permission_risk.get('high_risk_permissions', 0),
                "timestamp": datetime.now().isoformat()
            }
        }

    async def classify_smart_contract_vulnerability(self, move_code: str) -> Dict:
        """
        ä½¿ç”¨ LoRA å¾®èª¿çš„ Llama-2 æ¨¡å‹å° Move æ™ºèƒ½åˆç´„ä»£ç¢¼é€²è¡Œæ¼æ´åˆ†é¡
        åˆ†é¡ç‚ºï¼šreentrancy, overflow, access_control, logic_error, randomness_error, safe
        """
        try:
            if not self.model or not self.tokenizer:
                return {
                    "classification": "safe",
                    "confidence": 0.0,
                    "reasoning": "æ¨¡å‹æœªåˆå§‹åŒ–",
                    "error": "Model not initialized"
                }
            
            # æ§‹å»ºæç¤ºè©
            instruction = "è«‹æ‰¾å‡ºSui smart contractçš„æƒ¡æ„æ¼æ´ï¼Œä¸¦åˆ¤æ–·å±éšªç­‰ç´š"
            prompt = f"{instruction}\n{move_code}"
            
            # åˆ†è©
            input_ids = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=1024,
                truncation=True
            ).input_ids
            
            # ç§»è‡³ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                input_ids = input_ids.to('cuda')
            
            # ç”Ÿæˆé æ¸¬
            with torch.no_grad():
                output_ids = self.model.generate(
                    input_ids=input_ids,
                    max_new_tokens=128,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç¢¼è¼¸å‡º
            output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            
            # æå–åˆ†é¡æ¨™ç±¤
            classification = self.extract_label(output_text)
            
            # è§£æå±éšªç­‰ç´š
            confidence = 0.5  # é»˜èªä¿¡å¿ƒåº¦
            if "å±éšªç­‰ç´šï¼šé«˜" in output_text or "é«˜é¢¨éšª" in output_text:
                confidence = 0.9
                risk_level = "HIGH"
            elif "å±éšªç­‰ç´šï¼šä¸­" in output_text or "ä¸­é¢¨éšª" in output_text:
                confidence = 0.6
                risk_level = "MEDIUM"
            elif "å±éšªç­‰ç´šï¼šä½" in output_text or "ä½é¢¨éšª" in output_text:
                confidence = 0.3
                risk_level = "LOW"
            else:
                risk_level = "UNKNOWN"
            
            # æ˜ å°„åˆ°æ¨™æº–åˆ†é¡
            classification_map = {
                "reentrancy": "access_control",
                "overflow": "logic_error",
                "access_control": "access_control",
                "logic_error": "logic_error",
                "randomness_error": "randomness_error",
                "safe": "safe",
                "other_bug": "logic_error"
            }
            
            standard_classification = classification_map.get(classification, "safe")
            
            # æ§‹å»ºæ¦‚ç‡åˆ†å¸ƒ
            probabilities = {
                "access_control": 0.0,
                "logic_error": 0.0,
                "randomness_error": 0.0,
                "safe": 0.0
            }
            
            # æ ¹æ“šåˆ†é¡è¨­ç½®æ¦‚ç‡
            if standard_classification in probabilities:
                probabilities[standard_classification] = confidence
                # åˆ†é…å‰©é¤˜æ¦‚ç‡çµ¦å…¶ä»–é¡åˆ¥
                remaining_prob = 1.0 - confidence
                other_count = len(probabilities) - 1
                for key in probabilities:
                    if key != standard_classification:
                        probabilities[key] = remaining_prob / other_count
            else:
                probabilities["safe"] = 1.0
            
            # è¨ˆç®—é¢¨éšªåˆ†æ•¸
            classification_result = {
                "classification": standard_classification,
                "probabilities": probabilities,
                "max_probability": confidence,
                "reasoning": output_text,
                "original_classification": classification,
                "risk_level": risk_level
            }
            
            risk_score = self._calculate_probability_based_risk_score(classification_result)
            classification_result["risk_score"] = risk_score
            
            return classification_result
                        
        except Exception as e:
            return {
                "classification": "safe",
                "probabilities": {
                    "access_control": 0.0,
                    "logic_error": 0.0,
                    "randomness_error": 0.0,
                    "safe": 1.0
                },
                "max_probability": 1.0,
                "risk_score": 0,
                "reasoning": f"ML classification failed: {str(e)}",
                "error": f"Exception: {str(e)}"
            }

    def _calculate_probability_based_risk_score(self, ml_result: Dict) -> int:
        """
        åŸºæ–¼æ©Ÿç‡åˆ†å¸ƒè¨ˆç®—100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸
        
        è¨ˆç®—ç­–ç•¥ï¼š
        1. å–æœ€é«˜æ©Ÿç‡çš„æ¼æ´é¡å‹
        2. æ ¹æ“šæ©Ÿç‡é«˜ä½åœ¨è©²é¡å‹åˆ†æ•¸å€é–“å…§è¨ˆç®—å…·é«”åˆ†æ•¸
        3. è€ƒæ…®ä¿¡å¿ƒåº¦èª¿æ•´æœ€çµ‚åˆ†æ•¸
        """
        try:
            classification = ml_result.get("classification", "safe")
            probabilities = ml_result.get("probabilities", {})
            max_probability = ml_result.get("max_probability", 0.0)
            
            # ç²å–è©²åˆ†é¡çš„åˆ†æ•¸å€é–“
            score_range = self.vulnerability_score_ranges.get(classification, (0, 19))
            min_score, max_score = score_range
            
            # åŸºæ–¼æœ€é«˜æ©Ÿç‡è¨ˆç®—åœ¨å€é–“å…§çš„å…·é«”åˆ†æ•¸
            # æ©Ÿç‡è¶Šé«˜ï¼Œåˆ†æ•¸è¶Šæ¥è¿‘å€é–“ä¸Šé™
            base_score = min_score + (max_score - min_score) * max_probability
            
            # ä¿¡å¿ƒåº¦èª¿æ•´
            confidence_adjustment = self._get_confidence_adjustment(max_probability)
            final_score = base_score * confidence_adjustment
            
            # å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š (è€ƒæ…®å…¶ä»–é¡åˆ¥çš„å½±éŸ¿)
            weighted_score = self._apply_multi_class_weighting(probabilities, final_score)
            
            return int(round(min(max(weighted_score, 0), 100)))
            
        except Exception as e:
            print(f"é¢¨éšªåˆ†æ•¸è¨ˆç®—éŒ¯èª¤: {e}")
            return 0

    def _get_confidence_adjustment(self, max_probability: float) -> float:
        """æ ¹æ“šä¿¡å¿ƒåº¦èª¿æ•´åˆ†æ•¸ä¿‚æ•¸"""
        if max_probability >= self.confidence_thresholds["high_confidence"]:
            return 1.0  # é«˜ä¿¡å¿ƒåº¦ï¼Œä¸èª¿æ•´
        elif max_probability >= self.confidence_thresholds["medium_confidence"]:
            return 0.8  # ä¸­ä¿¡å¿ƒåº¦ï¼Œé©åº¦é™ä½
        elif max_probability >= self.confidence_thresholds["low_confidence"]:
            return 0.6  # ä½ä¿¡å¿ƒåº¦ï¼Œæ˜é¡¯é™ä½
        else:
            # æ¥µä½ä¿¡å¿ƒåº¦ï¼šå‚¾å‘æ–¼å ±å‘Šç‚º"é¢¨éšªä¸æ˜"çš„ä¸­ä½åˆ†æ•¸å€åŸŸ
            return 0.3  # å¤§å¹…é™ä½ï¼Œé¿å…é«˜é¢¨éšªèª¤å ±

    def _apply_multi_class_weighting(self, probabilities: Dict, base_score: float) -> float:
        """
        æ‡‰ç”¨å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š
        è€ƒæ…®å…¶ä»–æ¼æ´é¡å‹çš„æ©Ÿç‡å°æœ€çµ‚åˆ†æ•¸çš„å½±éŸ¿
        """
        try:
            # è¨ˆç®—åŠ æ¬Šé¢¨éšªè²¢ç»
            weighted_contribution = 0.0
            
            for vuln_type, probability in probabilities.items():
                if vuln_type == "safe":
                    continue  # è·³éå®‰å…¨é¡åˆ¥
                
                # ç²å–è©²æ¼æ´é¡å‹çš„ä¸­ä½åˆ†æ•¸
                score_range = self.vulnerability_score_ranges.get(vuln_type, (0, 19))
                mid_score = (score_range[0] + score_range[1]) / 2
                
                # åŠ æ¬Šè²¢ç» = æ©Ÿç‡ Ã— è©²é¡å‹ä¸­ä½åˆ†æ•¸
                weighted_contribution += probability * mid_score
            
            # çµåˆåŸºç¤åˆ†æ•¸å’ŒåŠ æ¬Šè²¢ç» (70% åŸºç¤ + 30% åŠ æ¬Š)
            final_score = (base_score * 0.7) + (weighted_contribution * 0.3)
            
            return final_score
            
        except Exception as e:
            print(f"å¤šé¡åˆ¥åŠ æ¬Šè¨ˆç®—éŒ¯èª¤: {e}")
            return base_score

    def _convert_score_to_risk_level(self, risk_score: int) -> tuple:
        """
        å°‡100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸è½‰æ›ç‚ºé¢¨éšªç­‰ç´šå’Œå»ºè­°
        
        Args:
            risk_score: 0-100çš„é¢¨éšªåˆ†æ•¸
            
        Returns:
            tuple: (risk_level, recommendation, normalized_score)
        """
        # å°‡100åˆ†åˆ¶è½‰æ›ç‚º0-1ç¯„åœ (ç”¨æ–¼ç›¸å®¹æ€§)
        normalized_score = risk_score / 100.0
        
        if risk_score >= 70:
            return "HIGH", "ğŸš« æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª (MLåˆ†æ)", normalized_score
        elif risk_score >= 40:
            return "MEDIUM", "âš ï¸ è­¦å‘Š - è«‹è¬¹æ…è™•ç† (MLåˆ†æ)", normalized_score
        elif risk_score >= 20:
            return "LOW", "âœ… å¯æ¥å— - æª¢æ¸¬åˆ°ä½é¢¨éšª (MLåˆ†æ)", normalized_score
        else:
            return "SAFE", "âœ… æ‰¹å‡† - æœªæª¢æ¸¬åˆ°æ˜é¡¯é¢¨éšª (MLåˆ†æ)", normalized_score

    async def analyze_with_ml_integration(self, domain: str, permissions: List[str], 
                                        package_analyses: List[Dict], move_source_code: str = "") -> Dict:
        """
        çµåˆè¦å‰‡å¼•æ“å’Œæ©Ÿå™¨å­¸ç¿’çš„ç¶œåˆé¢¨éšªåˆ†æ
        """
        try:
            # åŸºç¤è¦å‰‡å¼•æ“åˆ†æ
            rule_based_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            
            # æ©Ÿå™¨å­¸ç¿’æ™ºèƒ½åˆç´„æ¼æ´åˆ†é¡
            ml_classification = None
            ml_risk_score = 0.0
            
            if move_source_code.strip():
                ml_classification = await self.classify_smart_contract_vulnerability(move_source_code)
                # ä½¿ç”¨æ–°çš„100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸ (è½‰æ›ç‚º0-1ç¯„åœ)
                ml_risk_score = ml_classification.get('risk_score', 0) / 100.0
            
            # åˆä½µé¢¨éšªåˆ†æçµæœ
            rule_risk_score = rule_based_analysis['risk_breakdown']['final_score']
            
            # è¨ˆç®—ç¶œåˆé¢¨éšªåˆ†æ•¸
            if ml_classification and ml_classification.get('confidence', 0) > 0.3:
                # ML åˆ†é¡å¯ä¿¡åº¦é«˜æ™‚ï¼Œçµ¦äºˆæ›´é«˜æ¬Šé‡
                final_risk_score = (ml_risk_score * 0.6) + (rule_risk_score * 0.4)
                confidence_boost = 0.1
            else:
                # ML åˆ†é¡ä¸å¯ç”¨æˆ–å¯ä¿¡åº¦ä½æ™‚ï¼Œä¸»è¦ä¾è³´è¦å‰‡å¼•æ“
                final_risk_score = (rule_risk_score * 0.8) + (ml_risk_score * 0.2)
                confidence_boost = 0.0
            
            # é‡æ–°ç¢ºå®šé¢¨éšªç­‰ç´š
            if final_risk_score >= 0.7:
                risk_level = "HIGH"
                recommendation = "æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšªï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            elif final_risk_score >= 0.4:
                risk_level = "MEDIUM"
                recommendation = "è­¦å‘Š - è«‹è¬¹æ…è™•ç†ï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            else:
                risk_level = "LOW"
                recommendation = "æ‰¹å‡† - æª¢æ¸¬åˆ°ä½é¢¨éšªï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            
            # åˆä½µé¢¨éšªåŸå› 
            all_reasons = rule_based_analysis['reasons'].copy()
            if ml_classification and ml_classification.get('classification') != 'safe':
                all_reasons.append(
                    f"MLæª¢æ¸¬åˆ°æ™ºèƒ½åˆç´„æ¼æ´: {ml_classification['classification']} "
                    f"(ä¿¡å¿ƒåº¦: {ml_classification.get('confidence', 0):.2f})"
                )
            
            return {
                "risk_level": risk_level,
                "confidence": round(final_risk_score + confidence_boost, 2),
                "reasons": all_reasons,
                "recommendation": recommendation,
                "risk_breakdown": {
                    **rule_based_analysis['risk_breakdown'],
                    "ml_vulnerability_score": round(ml_risk_score, 2),
                    "final_combined_score": round(final_risk_score, 2)
                },
                "ml_analysis": ml_classification,
                "details": {
                    **rule_based_analysis['details'],
                    "ml_enabled": bool(move_source_code.strip()),
                    "analysis_method": "hybrid_ml_rules",
                    "ml_risk_score_100": ml_classification.get('risk_score', 0) if ml_classification else 0,
                    "ml_probabilities": ml_classification.get('probabilities', {}) if ml_classification else {}
                }
            }
            
        except Exception as e:
            # å¦‚æœMLåˆ†æå¤±æ•—ï¼Œå›é€€åˆ°ç´”è¦å‰‡å¼•æ“
            rule_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            rule_analysis['details']['ml_analysis_error'] = str(e)
            rule_analysis['details']['analysis_method'] = "rules_only_fallback"
            return rule_analysis
