from typing import List, Dict, Optional
import re
from datetime import datetime
import json
import aiohttp
import os

class RiskEngine:
    """é¢¨éšªè©•ä¼°å¼•æ“ - 
    è² è²¬åˆ†æåŸŸåã€æ¬Šé™å’Œæ™ºèƒ½åˆç´„åŒ…çš„é¢¨éšªç­‰ç´š
    æä¾›ç¶œåˆæ€§çš„å®‰å…¨é¢¨éšªè©•ä¼°å’Œå»ºè­°
    """
    
    def __init__(self):
        # DeepSeek-R1 API é…ç½®
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        self.deepseek_endpoint = "https://api.deepseek.com/v1/chat/completions"
        
        # æ¼æ´åˆ†é¡æ˜ å°„åˆ°é¢¨éšªåˆ†æ•¸å€é–“ (100åˆ†åˆ¶)
        self.vulnerability_score_ranges = {
            "access_control": (80, 100),    # å­˜å–æ§åˆ¶æ¼æ´ - é«˜é¢¨éšª (80-100åˆ†)
            "logic_error": (50, 79),        # é‚è¼¯éŒ¯èª¤æ¼æ´ - ä¸­é¢¨éšª (50-79åˆ†)  
            "randomness_error": (20, 49),   # éš¨æ©Ÿæ•¸æ¼æ´ - ä½é¢¨éšª (20-49åˆ†)
            "safe": (0, 19)                 # å®‰å…¨ä»£ç¢¼ - ç„¡é¢¨éšª (0-19åˆ†)
        }
        
        # æ©Ÿç‡åˆ†å¸ƒé–¾å€¼é…ç½®
        self.confidence_thresholds = {
            "high_confidence": 0.8,      # é«˜ä¿¡å¿ƒåº¦é–¾å€¼
            "medium_confidence": 0.6,    # ä¸­ä¿¡å¿ƒåº¦é–¾å€¼
            "low_confidence": 0.4        # ä½ä¿¡å¿ƒåº¦é–¾å€¼
        }
        
        # æƒ¡æ„åŸŸåé—œéµå­—æ¸…å–®
        self.malicious_domains = {
            'phishing', 'fake', 'scam', 'steal', 'malicious', 'hack', 
            'fraud', 'theft', 'fishing', 'wallet-stealer', 'crypto-steal',
            'bitcoin-scam', 'eth-fake', 'sui-fake', 'defi-scam', 'nft-steal',
            'metamask-fake', 'phantom-fake', 'ledger-fake', 'trezor-fake'
        }
        
        # å¯ç–‘åŸŸåé—œéµå­—æ¸…å–®
        self.suspicious_domains = {
            'free', 'bonus', 'gift', 'earn', 'quick', 'fast', 'easy',
            'double', 'triple', 'profit', 'money', 'rich', 'millionaire',
            'lottery', 'winner', 'prize', 'reward', 'airdrop-free'
        }
        
        # é«˜é¢¨éšªæ¬Šé™æ¸…å–®
        self.high_risk_permissions = {
            'wallet:sign', 'wallet:transfer', 'wallet:approve_all',
            'wallet:delegate', 'wallet:admin'
        }
        
        # ä¸­ç­‰é¢¨éšªæ¬Šé™æ¸…å–®
        self.medium_risk_permissions = {
            'wallet:read_balance', 'wallet:read_history', 'wallet:connect'
        }
        
        # å·²çŸ¥å®‰å…¨çš„å®˜æ–¹åŸŸå
        self.trusted_domains = {
            'sui.io', 'mysten.io', 'suiwallet.com', 'ethoswallet.com',
            'martianwallet.xyz', 'github.com', 'chrome.google.com'
        }
        
        # å®˜æ–¹SuiåŒ…åœ°å€
        self.official_sui_packages = {
            "0x0000000000000000000000000000000000000000000000000000000000000001",  # Move stdlib
            "0x0000000000000000000000000000000000000000000000000000000000000002",  # Sui framework
            "0x0000000000000000000000000000000000000000000000000000000000000003"   # Sui system
        }
    
    def analyze_domain_risk(self, domain: str) -> Dict:
        """åˆ†æåŸŸåé¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        domain_lower = domain.lower()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºä¿¡ä»»åŸŸå
        for trusted in self.trusted_domains:
            if trusted in domain_lower:
                return {
                    "risk_score": 0.0,
                    "reasons": [f"ä¿¡ä»»åŸŸå: {trusted}"]
                }
        
        # æª¢æŸ¥æƒ¡æ„é—œéµå­—
        for keyword in self.malicious_domains:
            if keyword in domain_lower:
                risk_score += 0.8
                reasons.append(f"é«˜é¢¨éšªåŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥å¯ç–‘é—œéµå­—  
        for keyword in self.suspicious_domains:
            if keyword in domain_lower:
                risk_score += 0.3
                reasons.append(f"å¯ç–‘åŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥åŸŸåé•·åº¦ç•°å¸¸
        if len(domain) > 30:
            risk_score += 0.2
            reasons.append("åŸŸåé•·åº¦ç•°å¸¸")
        
        # æª¢æŸ¥éå¤šé€£å­—ç¬¦
        if domain.count('-') > 2:
            risk_score += 0.3
            reasons.append("åŸŸååŒ…å«éå¤šé€£å­—ç¬¦")
        
        # æª¢æŸ¥æ•¸å­—å­—æ¯æ··åˆæ¨¡å¼
        if any(c.isdigit() for c in domain) and any(c.isalpha() for c in domain):
            digit_count = sum(c.isdigit() for c in domain)
            if digit_count > 3:
                risk_score += 0.2
                reasons.append("å¯ç–‘çš„æ•¸å­—å­—æ¯çµ„åˆ")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons
        }
    
    def analyze_permissions_risk(self, permissions: List[str]) -> Dict:
        """åˆ†ææ¬Šé™é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        high_risk_count = 0
        medium_risk_count = 0
        
        for permission in permissions:
            if permission in self.high_risk_permissions:
                risk_score += 0.4
                high_risk_count += 1
                reasons.append(f"é«˜é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
            elif permission in self.medium_risk_permissions:
                risk_score += 0.2
                medium_risk_count += 1
                reasons.append(f"ä¸­ç­‰é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
        
        # æ¬Šé™æ•¸é‡é¢¨éšªè©•ä¼°
        total_permissions = len(permissions)
        if total_permissions > 5:
            risk_score += 0.3
            reasons.append(f"è«‹æ±‚éå¤šæ¬Šé™: {total_permissions}å€‹")
        elif total_permissions > 3:
            risk_score += 0.1
            reasons.append(f"è«‹æ±‚è¼ƒå¤šæ¬Šé™: {total_permissions}å€‹")
        
        # é«˜é¢¨éšªæ¬Šé™çµ„åˆæª¢æŸ¥
        if 'wallet:sign' in permissions and 'wallet:transfer' in permissions:
            risk_score += 0.2
            reasons.append("å±éšªæ¬Šé™çµ„åˆ: ç°½å+è½‰å¸³")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "high_risk_permissions": high_risk_count,
            "medium_risk_permissions": medium_risk_count
        }
    
    def analyze_package_risk(self, package_analyses: List[Dict]) -> Dict:
        """åˆ†ææ™ºèƒ½åˆç´„åŒ…é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        analyzed_count = 0
        
        for analysis in package_analyses:
            if analysis.get('status') != 'success':
                continue
                
            analyzed_count += 1
            pkg_analysis = analysis.get('analysis', {})
            package_id = pkg_analysis.get('package_id', '')
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå®˜æ–¹SuiåŒ…
            if package_id in self.official_sui_packages:
                reasons.append("å®˜æ–¹Suiå¥—ä»¶ - å·²é©—è­‰å®‰å…¨")
                continue
            
            # åˆ†æå±éšªå‡½æ•¸
            dangerous_functions = pkg_analysis.get('dangerous_functions', [])
            if len(dangerous_functions) > 10:
                risk_score += 0.4
                reasons.append(f"æª¢æ¸¬åˆ°å¤§é‡å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
            elif len(dangerous_functions) > 5:
                risk_score += 0.2
                reasons.append(f"æª¢æ¸¬åˆ°å¤šå€‹å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "analyzed_packages": analyzed_count
        }
    
    def calculate_overall_risk(self, domain: str, permissions: List[str], package_analyses: List[Dict]) -> Dict:
        """ç¶œåˆé¢¨éšªè©•ä¼° - ä¸»è¦æ–¹æ³•"""
        
        # å„é …é¢¨éšªåˆ†æ
        domain_risk = self.analyze_domain_risk(domain)
        permission_risk = self.analyze_permissions_risk(permissions)
        package_risk = self.analyze_package_risk(package_analyses)
        
        # è¨ˆç®—åŠ æ¬Šé¢¨éšªåˆ†æ•¸
        # åŸŸåé¢¨éšªæ¬Šé‡æœ€é«˜ï¼Œå› ç‚ºæƒ¡æ„åŸŸåé€šå¸¸æ˜¯æœ€æ˜é¡¯çš„å±éšªä¿¡è™Ÿ
        domain_weight = 0.5
        permission_weight = 0.3
        package_weight = 0.2
        
        weighted_risk = (
            domain_risk['risk_score'] * domain_weight +
            permission_risk['risk_score'] * permission_weight +
            package_risk['risk_score'] * package_weight
        )
        
        # å¦‚æœä»»ä¸€é …ç›®é¢¨éšªæ¥µé«˜ï¼Œå‰‡ç¸½é¢¨éšªä¹Ÿæ‡‰è©²å¾ˆé«˜
        max_individual_risk = max(
            domain_risk['risk_score'],
            permission_risk['risk_score'],
            package_risk['risk_score']
        )
        
        # ä½¿ç”¨åŠ æ¬Šå¹³å‡å’Œæœ€é«˜å€‹åˆ¥é¢¨éšªçš„è¼ƒå¤§å€¼
        total_risk = max(weighted_risk, max_individual_risk * 0.8)
        
        # åˆä½µæ‰€æœ‰é¢¨éšªåŸå› 
        all_reasons = (
            domain_risk['reasons'] + 
            permission_risk['reasons'] + 
            package_risk['reasons']
        )
        
        # ç¢ºå®šé¢¨éšªç­‰ç´šå’Œå»ºè­°
        if total_risk >= 0.7:
            risk_level = "HIGH"
            recommendation = "æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª"
        elif total_risk >= 0.4:
            risk_level = "MEDIUM" 
            recommendation = "è­¦å‘Š - è«‹è¬¹æ…è™•ç†"
        else:
            risk_level = "LOW"
            recommendation = "æ‰¹å‡† - æª¢æ¸¬åˆ°ä½é¢¨éšª"
        
        return {
            "risk_level": risk_level,
            "confidence": round(total_risk, 2),
            "reasons": all_reasons,
            "recommendation": recommendation,
            "risk_breakdown": {
                "domain_risk": round(domain_risk['risk_score'], 2),
                "permission_risk": round(permission_risk['risk_score'], 2),
                "package_risk": round(package_risk['risk_score'], 2),
                "weighted_score": round(weighted_risk, 2),
                "final_score": round(total_risk, 2)
            },
            "details": {
                "analyzed_packages": package_risk['analyzed_packages'],
                "high_risk_permissions": permission_risk.get('high_risk_permissions', 0),
                "timestamp": datetime.now().isoformat()
            }
        }

    async def classify_smart_contract_vulnerability(self, move_code: str) -> Dict:
        """
        ä½¿ç”¨ DeepSeek-R1 å° Move æ™ºèƒ½åˆç´„ä»£ç¢¼é€²è¡Œæ¼æ´åˆ†é¡
        åˆ†é¡ç‚ºï¼šaccess_control, logic_error, randomness_error, safe
        """
        try:
            if not self.deepseek_api_key:
                return {
                    "classification": "safe",
                    "confidence": 0.0,
                    "reasoning": "DeepSeek API key not configured",
                    "error": "API key missing"
                }
            
            # æ§‹å»ºåˆ†é¡æç¤ºè© - è¦æ±‚æ©Ÿç‡åˆ†å¸ƒè¼¸å‡º
            prompt = f"""è«‹åˆ†æä»¥ä¸‹ Move æ™ºèƒ½åˆç´„ä»£ç¢¼ï¼Œåˆ¤æ–·å…¶å®‰å…¨æ¼æ´é¡å‹ã€‚

æ¼æ´åˆ†é¡å®šç¾©ï¼š
1. access_control - å­˜å–æ§åˆ¶æ¼æ´ï¼ˆæ¬Šé™æª¢æŸ¥ä¸ç•¶ã€æœªæˆæ¬Šæ“ä½œç­‰ï¼‰
2. logic_error - é‚è¼¯éŒ¯èª¤æ¼æ´ï¼ˆè¨ˆç®—éŒ¯èª¤ã€ç‹€æ…‹ç®¡ç†å•é¡Œç­‰ï¼‰  
3. randomness_error - éš¨æ©Ÿæ•¸æ¼æ´ï¼ˆå¯é æ¸¬éš¨æ©Ÿæ•¸ã€å½éš¨æ©Ÿæ•¸ç­‰ï¼‰

è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼ŒåŒ…å«ï¼š
- classification: æœ€å¯èƒ½çš„åˆ†é¡ (access_control/logic_error/randomness_error/safe)
- probabilities: å„é¡åˆ¥æ©Ÿç‡åˆ†å¸ƒ
  - access_control: 0.0-1.0
  - logic_error: 0.0-1.0  
  - randomness_error: 0.0-1.0
  - safe: 0.0-1.0
- max_probability: æœ€é«˜æ©Ÿç‡å€¼ (0.0-1.0)
- reasoning: è©³ç´°åˆ†æåŸå› 

æ³¨æ„ï¼šå››å€‹æ©Ÿç‡ç¸½å’Œæ‡‰æ¥è¿‘1.0

Move ä»£ç¢¼ï¼š
```move
{move_code}
```

è«‹åƒ…å›ç­” JSON æ ¼å¼ã€‚"""

            # èª¿ç”¨ DeepSeek API
            headers = {
                "Authorization": f"Bearer {self.deepseek_api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-reasoner",
                "messages": [
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "temperature": 0.1,
                "max_tokens": 1000
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.deepseek_endpoint, 
                    headers=headers, 
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        
                        # è§£æ JSON å›æ‡‰
                        try:
                            classification_result = json.loads(content)
                            
                            # é©—è­‰æ–°çš„å›æ‡‰æ ¼å¼
                            required_fields = ["classification", "probabilities", "max_probability", "reasoning"]
                            if all(field in classification_result for field in required_fields):
                                # è¨ˆç®—åŸºæ–¼æ©Ÿç‡çš„é¢¨éšªåˆ†æ•¸
                                risk_score = self._calculate_probability_based_risk_score(classification_result)
                                classification_result["risk_score"] = risk_score
                                return classification_result
                            else:
                                return {
                                    "classification": "safe",
                                    "probabilities": {"access_control": 0.0, "logic_error": 0.0, "randomness_error": 0.0, "safe": 1.0},
                                    "max_probability": 1.0,
                                    "risk_score": 0,
                                    "reasoning": f"Invalid response format from DeepSeek: {content}",
                                    "error": "Invalid response format"
                                }
                                
                        except json.JSONDecodeError:
                            return {
                                "classification": "safe",
                                "probabilities": {"access_control": 0.0, "logic_error": 0.0, "randomness_error": 0.0, "safe": 1.0},
                                "max_probability": 1.0,
                                "risk_score": 0,
                                "reasoning": f"Failed to parse DeepSeek response: {content}",
                                "error": "JSON parse error"
                            }
                    else:
                        error_text = await response.text()
                        return {
                            "classification": "safe", 
                            "confidence": 0.0,
                            "reasoning": f"DeepSeek API error: {response.status} - {error_text}",
                            "error": f"API error {response.status}"
                        }
                        
        except Exception as e:
            return {
                "classification": "safe",
                "probabilities": {"access_control": 0.0, "logic_error": 0.0, "randomness_error": 0.0, "safe": 1.0},
                "max_probability": 1.0,
                "risk_score": 0,
                "reasoning": f"ML classification failed: {str(e)}",
                "error": f"Exception: {str(e)}"
            }

    def _calculate_probability_based_risk_score(self, ml_result: Dict) -> int:
        """
        åŸºæ–¼æ©Ÿç‡åˆ†å¸ƒè¨ˆç®—100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸
        
        è¨ˆç®—ç­–ç•¥ï¼š
        1. å–æœ€é«˜æ©Ÿç‡çš„æ¼æ´é¡å‹
        2. æ ¹æ“šæ©Ÿç‡é«˜ä½åœ¨è©²é¡å‹åˆ†æ•¸å€é–“å…§è¨ˆç®—å…·é«”åˆ†æ•¸
        3. è€ƒæ…®ä¿¡å¿ƒåº¦èª¿æ•´æœ€çµ‚åˆ†æ•¸
        """
        try:
            classification = ml_result.get("classification", "safe")
            probabilities = ml_result.get("probabilities", {})
            max_probability = ml_result.get("max_probability", 0.0)
            
            # ç²å–è©²åˆ†é¡çš„åˆ†æ•¸å€é–“
            score_range = self.vulnerability_score_ranges.get(classification, (0, 19))
            min_score, max_score = score_range
            
            # åŸºæ–¼æœ€é«˜æ©Ÿç‡è¨ˆç®—åœ¨å€é–“å…§çš„å…·é«”åˆ†æ•¸
            # æ©Ÿç‡è¶Šé«˜ï¼Œåˆ†æ•¸è¶Šæ¥è¿‘å€é–“ä¸Šé™
            base_score = min_score + (max_score - min_score) * max_probability
            
            # ä¿¡å¿ƒåº¦èª¿æ•´
            confidence_adjustment = self._get_confidence_adjustment(max_probability)
            final_score = base_score * confidence_adjustment
            
            # å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š (è€ƒæ…®å…¶ä»–é¡åˆ¥çš„å½±éŸ¿)
            weighted_score = self._apply_multi_class_weighting(probabilities, final_score)
            
            return int(round(min(max(weighted_score, 0), 100)))
            
        except Exception as e:
            print(f"é¢¨éšªåˆ†æ•¸è¨ˆç®—éŒ¯èª¤: {e}")
            return 0

    def _get_confidence_adjustment(self, max_probability: float) -> float:
        """æ ¹æ“šä¿¡å¿ƒåº¦èª¿æ•´åˆ†æ•¸ä¿‚æ•¸"""
        if max_probability >= self.confidence_thresholds["high_confidence"]:
            return 1.0  # é«˜ä¿¡å¿ƒåº¦ï¼Œä¸èª¿æ•´
        elif max_probability >= self.confidence_thresholds["medium_confidence"]:
            return 0.8  # ä¸­ä¿¡å¿ƒåº¦ï¼Œé©åº¦é™ä½
        elif max_probability >= self.confidence_thresholds["low_confidence"]:
            return 0.6  # ä½ä¿¡å¿ƒåº¦ï¼Œæ˜é¡¯é™ä½
        else:
            # æ¥µä½ä¿¡å¿ƒåº¦ï¼šå‚¾å‘æ–¼å ±å‘Šç‚º"é¢¨éšªä¸æ˜"çš„ä¸­ä½åˆ†æ•¸å€åŸŸ
            return 0.3  # å¤§å¹…é™ä½ï¼Œé¿å…é«˜é¢¨éšªèª¤å ±

    def _apply_multi_class_weighting(self, probabilities: Dict, base_score: float) -> float:
        """
        æ‡‰ç”¨å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š
        è€ƒæ…®å…¶ä»–æ¼æ´é¡å‹çš„æ©Ÿç‡å°æœ€çµ‚åˆ†æ•¸çš„å½±éŸ¿
        """
        try:
            # è¨ˆç®—åŠ æ¬Šé¢¨éšªè²¢ç»
            weighted_contribution = 0.0
            
            for vuln_type, probability in probabilities.items():
                if vuln_type == "safe":
                    continue  # è·³éå®‰å…¨é¡åˆ¥
                
                # ç²å–è©²æ¼æ´é¡å‹çš„ä¸­ä½åˆ†æ•¸
                score_range = self.vulnerability_score_ranges.get(vuln_type, (0, 19))
                mid_score = (score_range[0] + score_range[1]) / 2
                
                # åŠ æ¬Šè²¢ç» = æ©Ÿç‡ Ã— è©²é¡å‹ä¸­ä½åˆ†æ•¸
                weighted_contribution += probability * mid_score
            
            # çµåˆåŸºç¤åˆ†æ•¸å’ŒåŠ æ¬Šè²¢ç» (70% åŸºç¤ + 30% åŠ æ¬Š)
            final_score = (base_score * 0.7) + (weighted_contribution * 0.3)
            
            return final_score
            
        except Exception as e:
            print(f"å¤šé¡åˆ¥åŠ æ¬Šè¨ˆç®—éŒ¯èª¤: {e}")
            return base_score

    def _convert_score_to_risk_level(self, risk_score: int) -> tuple:
        """
        å°‡100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸è½‰æ›ç‚ºé¢¨éšªç­‰ç´šå’Œå»ºè­°
        
        Args:
            risk_score: 0-100çš„é¢¨éšªåˆ†æ•¸
            
        Returns:
            tuple: (risk_level, recommendation, normalized_score)
        """
        # å°‡100åˆ†åˆ¶è½‰æ›ç‚º0-1ç¯„åœ (ç”¨æ–¼ç›¸å®¹æ€§)
        normalized_score = risk_score / 100.0
        
        if risk_score >= 70:
            return "HIGH", "ğŸš« æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª (MLåˆ†æ)", normalized_score
        elif risk_score >= 40:
            return "MEDIUM", "âš ï¸ è­¦å‘Š - è«‹è¬¹æ…è™•ç† (MLåˆ†æ)", normalized_score
        elif risk_score >= 20:
            return "LOW", "âœ… å¯æ¥å— - æª¢æ¸¬åˆ°ä½é¢¨éšª (MLåˆ†æ)", normalized_score
        else:
            return "SAFE", "âœ… æ‰¹å‡† - æœªæª¢æ¸¬åˆ°æ˜é¡¯é¢¨éšª (MLåˆ†æ)", normalized_score

    async def analyze_with_ml_integration(self, domain: str, permissions: List[str], 
                                        package_analyses: List[Dict], move_source_code: str = "") -> Dict:
        """
        çµåˆè¦å‰‡å¼•æ“å’Œæ©Ÿå™¨å­¸ç¿’çš„ç¶œåˆé¢¨éšªåˆ†æ
        """
        try:
            # åŸºç¤è¦å‰‡å¼•æ“åˆ†æ
            rule_based_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            
            # æ©Ÿå™¨å­¸ç¿’æ™ºèƒ½åˆç´„æ¼æ´åˆ†é¡
            ml_classification = None
            ml_risk_score = 0.0
            
            if move_source_code.strip():
                ml_classification = await self.classify_smart_contract_vulnerability(move_source_code)
                # ä½¿ç”¨æ–°çš„100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸ (è½‰æ›ç‚º0-1ç¯„åœ)
                ml_risk_score = ml_classification.get('risk_score', 0) / 100.0
            
            # åˆä½µé¢¨éšªåˆ†æçµæœ
            rule_risk_score = rule_based_analysis['risk_breakdown']['final_score']
            
            # è¨ˆç®—ç¶œåˆé¢¨éšªåˆ†æ•¸
            if ml_classification and ml_classification.get('confidence', 0) > 0.3:
                # ML åˆ†é¡å¯ä¿¡åº¦é«˜æ™‚ï¼Œçµ¦äºˆæ›´é«˜æ¬Šé‡
                final_risk_score = (ml_risk_score * 0.6) + (rule_risk_score * 0.4)
                confidence_boost = 0.1
            else:
                # ML åˆ†é¡ä¸å¯ç”¨æˆ–å¯ä¿¡åº¦ä½æ™‚ï¼Œä¸»è¦ä¾è³´è¦å‰‡å¼•æ“
                final_risk_score = (rule_risk_score * 0.8) + (ml_risk_score * 0.2)
                confidence_boost = 0.0
            
            # é‡æ–°ç¢ºå®šé¢¨éšªç­‰ç´š
            if final_risk_score >= 0.7:
                risk_level = "HIGH"
                recommendation = "æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšªï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            elif final_risk_score >= 0.4:
                risk_level = "MEDIUM"
                recommendation = "è­¦å‘Š - è«‹è¬¹æ…è™•ç†ï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            else:
                risk_level = "LOW"
                recommendation = "æ‰¹å‡† - æª¢æ¸¬åˆ°ä½é¢¨éšªï¼ˆML+è¦å‰‡å¼•æ“ï¼‰"
            
            # åˆä½µé¢¨éšªåŸå› 
            all_reasons = rule_based_analysis['reasons'].copy()
            if ml_classification and ml_classification.get('classification') != 'safe':
                all_reasons.append(
                    f"MLæª¢æ¸¬åˆ°æ™ºèƒ½åˆç´„æ¼æ´: {ml_classification['classification']} "
                    f"(ä¿¡å¿ƒåº¦: {ml_classification.get('confidence', 0):.2f})"
                )
            
            return {
                "risk_level": risk_level,
                "confidence": round(final_risk_score + confidence_boost, 2),
                "reasons": all_reasons,
                "recommendation": recommendation,
                "risk_breakdown": {
                    **rule_based_analysis['risk_breakdown'],
                    "ml_vulnerability_score": round(ml_risk_score, 2),
                    "final_combined_score": round(final_risk_score, 2)
                },
                "ml_analysis": ml_classification,
                "details": {
                    **rule_based_analysis['details'],
                    "ml_enabled": bool(move_source_code.strip()),
                    "analysis_method": "hybrid_ml_rules",
                    "ml_risk_score_100": ml_classification.get('risk_score', 0) if ml_classification else 0,
                    "ml_probabilities": ml_classification.get('probabilities', {}) if ml_classification else {}
                }
            }
            
        except Exception as e:
            # å¦‚æœMLåˆ†æå¤±æ•—ï¼Œå›é€€åˆ°ç´”è¦å‰‡å¼•æ“
            rule_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            rule_analysis['details']['ml_analysis_error'] = str(e)
            rule_analysis['details']['analysis_method'] = "rules_only_fallback"
            return rule_analysis
