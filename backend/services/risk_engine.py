from typing import List, Dict, Optional
import re
from datetime import datetime
import json
import aiohttp
import os
import logging

logger = logging.getLogger(__name__)

class RiskEngine:
    """é¢¨éšªè©•ä¼°å¼•æ“ - 
    è² è²¬åˆ†æåŸŸåã€æ¬Šé™å’Œæ™ºèƒ½åˆç´„åŒ…çš„é¢¨éšªç­‰ç´š
    æä¾›ç¶œåˆæ€§çš„å®‰å…¨é¢¨éšªè©•ä¼°å’Œå»ºè­°
    """
    
    def __init__(self):
        # ML æœå‹™é…ç½® (é€šé HTTP èª¿ç”¨ç¨ç«‹æœå‹™)
        self.ml_service_url = os.getenv("ML_SERVICE_URL", "http://localhost:8081")
        self.ml_service_enabled = os.getenv("ENABLE_ML_SERVICE", "true").lower() == "true"
        self.ml_service_timeout = int(os.getenv("ML_SERVICE_TIMEOUT", "30"))
        
        logger.info(f"ğŸ”§ RiskEngine åˆå§‹åŒ–: ML æœå‹™={'å•Ÿç”¨' if self.ml_service_enabled else 'ç¦ç”¨'}")
        if self.ml_service_enabled:
            logger.info(f"ğŸ”— ML æœå‹™ URL: {self.ml_service_url}")
        
        # æ¼æ´åˆ†é¡æ˜ å°„åˆ°é¢¨éšªåˆ†æ•¸å€é–“ (100åˆ†åˆ¶)
        self.vulnerability_score_ranges = {
            "access_control": (80, 100),    # å­˜å–æ§åˆ¶æ¼æ´ - é«˜é¢¨éšª (80-100åˆ†)
            "logic_error": (50, 79),        # é‚è¼¯éŒ¯èª¤æ¼æ´ - ä¸­é¢¨éšª (50-79åˆ†)  
            "randomness_error": (20, 49),   # éš¨æ©Ÿæ•¸æ¼æ´ - ä½é¢¨éšª (20-49åˆ†)
            "safe": (0, 19)                 # å®‰å…¨ä»£ç¢¼ - ç„¡é¢¨éšª (0-19åˆ†)
        }
        
        # æ©Ÿç‡åˆ†å¸ƒé–¾å€¼é…ç½®
        self.confidence_thresholds = {
            "high_confidence": 0.8,      # é«˜ä¿¡å¿ƒåº¦é–¾å€¼
            "medium_confidence": 0.6,    # ä¸­ä¿¡å¿ƒåº¦é–¾å€¼
            "low_confidence": 0.4        # ä½ä¿¡å¿ƒåº¦é–¾å€¼
        }
        
        # æƒ¡æ„åŸŸåé—œéµå­—æ¸…å–®
        self.malicious_domains = {
            'phishing', 'fake', 'scam', 'steal', 'malicious', 'hack', 
            'fraud', 'theft', 'fishing', 'wallet-stealer', 'crypto-steal',
            'bitcoin-scam', 'eth-fake', 'sui-fake', 'defi-scam', 'nft-steal',
            'metamask-fake', 'phantom-fake', 'ledger-fake', 'trezor-fake'
        }
        
        # å¯ç–‘åŸŸåé—œéµå­—æ¸…å–®
        self.suspicious_domains = {
            'free', 'bonus', 'gift', 'earn', 'quick', 'fast', 'easy',
            'double', 'triple', 'profit', 'money', 'rich', 'millionaire',
            'lottery', 'winner', 'prize', 'reward', 'airdrop-free'
        }
        
        # é«˜é¢¨éšªæ¬Šé™æ¸…å–®
        self.high_risk_permissions = {
            'wallet:sign', 'wallet:transfer', 'wallet:approve_all',
            'wallet:delegate', 'wallet:admin'
        }
        
        # ä¸­ç­‰é¢¨éšªæ¬Šé™æ¸…å–®
        self.medium_risk_permissions = {
            'wallet:read_balance', 'wallet:read_history', 'wallet:connect'
        }
        
        # å·²çŸ¥å®‰å…¨çš„å®˜æ–¹åŸŸå
        self.trusted_domains = {
            'sui.io', 'mysten.io', 'suiwallet.com', 'ethoswallet.com',
            'martianwallet.xyz', 'github.com', 'chrome.google.com'
        }
        
        # å®˜æ–¹SuiåŒ…åœ°å€
        self.official_sui_packages = {
            "0x0000000000000000000000000000000000000000000000000000000000000001",  # Move stdlib
            "0x0000000000000000000000000000000000000000000000000000000000000002",  # Sui framework
            "0x0000000000000000000000000000000000000000000000000000000000000003"   # Sui system
        }
    
    def analyze_domain_risk(self, domain: str) -> Dict:
        """åˆ†æåŸŸåé¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        domain_lower = domain.lower()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºä¿¡ä»»åŸŸå
        for trusted in self.trusted_domains:
            if trusted in domain_lower:
                return {
                    "risk_score": 0.0,
                    "reasons": [f"ä¿¡ä»»åŸŸå: {trusted}"]
                }
        
        # æª¢æŸ¥æƒ¡æ„é—œéµå­—
        for keyword in self.malicious_domains:
            if keyword in domain_lower:
                risk_score += 0.8
                reasons.append(f"é«˜é¢¨éšªåŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥å¯ç–‘é—œéµå­—  
        for keyword in self.suspicious_domains:
            if keyword in domain_lower:
                risk_score += 0.3
                reasons.append(f"å¯ç–‘åŸŸåæ¨¡å¼: {keyword}")
        
        # æª¢æŸ¥åŸŸåé•·åº¦ç•°å¸¸
        if len(domain) > 30:
            risk_score += 0.2
            reasons.append("åŸŸåé•·åº¦ç•°å¸¸")
        
        # æª¢æŸ¥éå¤šé€£å­—ç¬¦
        if domain.count('-') > 2:
            risk_score += 0.3
            reasons.append("åŸŸååŒ…å«éå¤šé€£å­—ç¬¦")
        
        # æª¢æŸ¥æ•¸å­—å­—æ¯æ··åˆæ¨¡å¼
        if any(c.isdigit() for c in domain) and any(c.isalpha() for c in domain):
            digit_count = sum(c.isdigit() for c in domain)
            if digit_count > 3:
                risk_score += 0.2
                reasons.append("å¯ç–‘çš„æ•¸å­—å­—æ¯çµ„åˆ")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons
        }
    
    def analyze_permissions_risk(self, permissions: List[str]) -> Dict:
        """åˆ†ææ¬Šé™é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        
        high_risk_count = 0
        medium_risk_count = 0
        
        for permission in permissions:
            if permission in self.high_risk_permissions:
                risk_score += 0.4
                high_risk_count += 1
                reasons.append(f"é«˜é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
            elif permission in self.medium_risk_permissions:
                risk_score += 0.2
                medium_risk_count += 1
                reasons.append(f"ä¸­ç­‰é¢¨éšªæ¬Šé™è«‹æ±‚: {permission}")
        
        # æ¬Šé™æ•¸é‡é¢¨éšªè©•ä¼°
        total_permissions = len(permissions)
        if total_permissions > 5:
            risk_score += 0.3
            reasons.append(f"è«‹æ±‚éå¤šæ¬Šé™: {total_permissions}å€‹")
        elif total_permissions > 3:
            risk_score += 0.1
            reasons.append(f"è«‹æ±‚è¼ƒå¤šæ¬Šé™: {total_permissions}å€‹")
        
        # é«˜é¢¨éšªæ¬Šé™çµ„åˆæª¢æŸ¥
        if 'wallet:sign' in permissions and 'wallet:transfer' in permissions:
            risk_score += 0.2
            reasons.append("å±éšªæ¬Šé™çµ„åˆ: ç°½å+è½‰å¸³")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "high_risk_permissions": high_risk_count,
            "medium_risk_permissions": medium_risk_count
        }
    
    def analyze_package_risk(self, package_analyses: List[Dict]) -> Dict:
        """åˆ†ææ™ºèƒ½åˆç´„åŒ…é¢¨éšª"""
        risk_score = 0.0
        reasons = []
        analyzed_count = 0
        
        for analysis in package_analyses:
            if analysis.get('status') != 'success':
                continue
                
            analyzed_count += 1
            pkg_analysis = analysis.get('analysis', {})
            package_id = pkg_analysis.get('package_id', '')
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå®˜æ–¹SuiåŒ…
            if package_id in self.official_sui_packages:
                reasons.append("å®˜æ–¹Suiå¥—ä»¶ - å·²é©—è­‰å®‰å…¨")
                continue
            
            # åˆ†æå±éšªå‡½æ•¸
            dangerous_functions = pkg_analysis.get('dangerous_functions', [])
            if len(dangerous_functions) > 10:
                risk_score += 0.4
                reasons.append(f"æª¢æ¸¬åˆ°å¤§é‡å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
            elif len(dangerous_functions) > 5:
                risk_score += 0.2
                reasons.append(f"æª¢æ¸¬åˆ°å¤šå€‹å±éšªå‡½æ•¸: {len(dangerous_functions)}å€‹")
        
        return {
            "risk_score": min(risk_score, 1.0),
            "reasons": reasons,
            "analyzed_packages": analyzed_count
        }
    
    def calculate_overall_risk(self, domain: str, permissions: List[str], package_analyses: List[Dict]) -> Dict:
        """ç¶œåˆé¢¨éšªè©•ä¼° - ä¸»è¦æ–¹æ³•"""
        
        # å„é …é¢¨éšªåˆ†æ
        domain_risk = self.analyze_domain_risk(domain)
        permission_risk = self.analyze_permissions_risk(permissions)
        package_risk = self.analyze_package_risk(package_analyses)
        
        # è¨ˆç®—åŠ æ¬Šé¢¨éšªåˆ†æ•¸
        # åŸŸåé¢¨éšªæ¬Šé‡æœ€é«˜ï¼Œå› ç‚ºæƒ¡æ„åŸŸåé€šå¸¸æ˜¯æœ€æ˜é¡¯çš„å±éšªä¿¡è™Ÿ
        domain_weight = 0.5
        permission_weight = 0.3
        package_weight = 0.2
        
        weighted_risk = (
            domain_risk['risk_score'] * domain_weight +
            permission_risk['risk_score'] * permission_weight +
            package_risk['risk_score'] * package_weight
        )
        
        # å¦‚æœä»»ä¸€é …ç›®é¢¨éšªæ¥µé«˜ï¼Œå‰‡ç¸½é¢¨éšªä¹Ÿæ‡‰è©²å¾ˆé«˜
        max_individual_risk = max(
            domain_risk['risk_score'],
            permission_risk['risk_score'],
            package_risk['risk_score']
        )
        
        # ä½¿ç”¨åŠ æ¬Šå¹³å‡å’Œæœ€é«˜å€‹åˆ¥é¢¨éšªçš„è¼ƒå¤§å€¼
        total_risk = max(weighted_risk, max_individual_risk * 0.8)
        
        # åˆä½µæ‰€æœ‰é¢¨éšªåŸå› 
        all_reasons = (
            domain_risk['reasons'] + 
            permission_risk['reasons'] + 
            package_risk['reasons']
        )
        
        # ç¢ºå®šé¢¨éšªç­‰ç´šå’Œå»ºè­°
        if total_risk >= 0.7:
            risk_level = "HIGH"
            recommendation = "æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª"
        elif total_risk >= 0.4:
            risk_level = "MEDIUM" 
            recommendation = "è­¦å‘Š - è«‹è¬¹æ…è™•ç†"
        else:
            risk_level = "LOW"
            recommendation = "æ‰¹å‡† - æª¢æ¸¬åˆ°ä½é¢¨éšª"
        
        return {
            "risk_level": risk_level,
            "confidence": round(total_risk, 2),
            "reasons": all_reasons,
            "recommendation": recommendation,
            "risk_breakdown": {
                "domain_risk": round(domain_risk['risk_score'], 2),
                "permission_risk": round(permission_risk['risk_score'], 2),
                "package_risk": round(package_risk['risk_score'], 2),
                "weighted_score": round(weighted_risk, 2),
                "final_score": round(total_risk, 2)
            },
            "details": {
                "analyzed_packages": package_risk['analyzed_packages'],
                "high_risk_permissions": permission_risk.get('high_risk_permissions', 0),
                "timestamp": datetime.now().isoformat()
            }
        }

    async def classify_smart_contract_vulnerability(self, move_code: str) -> Dict:
        """
        é€šé HTTP èª¿ç”¨ç¨ç«‹ ML æœå‹™é€²è¡Œæ™ºèƒ½åˆç´„æ¼æ´åˆ†é¡
        åˆ†é¡ç‚ºï¼šaccess_control, logic_error, randomness_error, safe
        """
        try:
            if not self.ml_service_enabled:
                logger.info("ML æœå‹™å·²ç¦ç”¨ï¼Œè¿”å›å®‰å…¨åˆ†é¡")
                return {
                    "classification": "safe",
                    "probabilities": {
                        "access_control": 0.0,
                        "logic_error": 0.0,
                        "randomness_error": 0.0,
                        "safe": 1.0
                    },
                    "max_probability": 1.0,
                    "risk_score": 0,
                    "risk_level": "SAFE",
                    "reasoning": "ML æœå‹™å·²ç¦ç”¨",
                    "service_status": "disabled"
                }
            
            # èª¿ç”¨ ML æœå‹™
            url = f"{self.ml_service_url}/api/analyze-vulnerability"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    url,
                    json={"move_code": move_code},
                    timeout=aiohttp.ClientTimeout(total=self.ml_service_timeout)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"âœ… ML æœå‹™åˆ†æå®Œæˆ: {result.get('classification')} (åˆ†æ•¸: {result.get('risk_score')})")
                        return result
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ ML æœå‹™è¿”å›éŒ¯èª¤: {response.status} - {error_text}")
                        raise Exception(f"ML service returned {response.status}")
                        
        except asyncio.TimeoutError:
            logger.warning("â±ï¸ ML æœå‹™è¶…æ™‚ï¼Œè¿”å›å®‰å…¨åˆ†é¡")
            return {
                "classification": "safe",
                "probabilities": {
                    "access_control": 0.0,
                    "logic_error": 0.0,
                    "randomness_error": 0.0,
                    "safe": 1.0
                },
                "max_probability": 1.0,
                "risk_score": 0,
                "risk_level": "SAFE",
                "reasoning": "ML æœå‹™è¶…æ™‚",
                "error": "timeout"
            }
        except Exception as e:
            logger.error(f"âŒ ML åˆ†é¡å¤±æ•—: {e}")
            return {
                "classification": "safe",
                "probabilities": {
                    "access_control": 0.0,
                    "logic_error": 0.0,
                    "randomness_error": 0.0,
                    "safe": 1.0
                },
                "max_probability": 1.0,
                "risk_score": 0,
                "risk_level": "SAFE",
                "reasoning": f"ML æœå‹™éŒ¯èª¤: {str(e)}",
                "error": str(e)
            }

    def _calculate_probability_based_risk_score(self, ml_result: Dict) -> int:
        """
        åŸºæ–¼æ©Ÿç‡åˆ†å¸ƒè¨ˆç®—100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸
        ï¼ˆæ³¨æ„ï¼šML æœå‹™å·²ç¶“è¨ˆç®—å¥½ risk_scoreï¼Œæ­¤æ–¹æ³•ç”¨æ–¼å…¼å®¹æ€§ï¼‰
        """
        # ML æœå‹™å·²ç¶“è¨ˆç®—å¥½é¢¨éšªåˆ†æ•¸ï¼Œç›´æ¥è¿”å›
        if "risk_score" in ml_result:
            return ml_result["risk_score"]
        
        # å›é€€é‚è¼¯ï¼ˆå¦‚æœ ML æœå‹™æœªæä¾› risk_scoreï¼‰
        try:
            classification = ml_result.get("classification", "safe")
            probabilities = ml_result.get("probabilities", {})
            max_probability = ml_result.get("max_probability", 0.0)
            
            # ç²å–è©²åˆ†é¡çš„åˆ†æ•¸å€é–“
            score_range = self.vulnerability_score_ranges.get(classification, (0, 19))
            min_score, max_score = score_range
            
            # åŸºæ–¼æœ€é«˜æ©Ÿç‡è¨ˆç®—åœ¨å€é–“å…§çš„å…·é«”åˆ†æ•¸
            # æ©Ÿç‡è¶Šé«˜ï¼Œåˆ†æ•¸è¶Šæ¥è¿‘å€é–“ä¸Šé™
            base_score = min_score + (max_score - min_score) * max_probability
            
            # ä¿¡å¿ƒåº¦èª¿æ•´
            confidence_adjustment = self._get_confidence_adjustment(max_probability)
            final_score = base_score * confidence_adjustment
            
            # å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š (è€ƒæ…®å…¶ä»–é¡åˆ¥çš„å½±éŸ¿)
            weighted_score = self._apply_multi_class_weighting(probabilities, final_score)
            
            return int(round(min(max(weighted_score, 0), 100)))
            
        except Exception as e:
            print(f"é¢¨éšªåˆ†æ•¸è¨ˆç®—éŒ¯èª¤: {e}")
            return 0

    def _get_confidence_adjustment(self, max_probability: float) -> float:
        """æ ¹æ“šä¿¡å¿ƒåº¦èª¿æ•´åˆ†æ•¸ä¿‚æ•¸"""
        if max_probability >= self.confidence_thresholds["high_confidence"]:
            return 1.0  # é«˜ä¿¡å¿ƒåº¦ï¼Œä¸èª¿æ•´
        elif max_probability >= self.confidence_thresholds["medium_confidence"]:
            return 0.8  # ä¸­ä¿¡å¿ƒåº¦ï¼Œé©åº¦é™ä½
        elif max_probability >= self.confidence_thresholds["low_confidence"]:
            return 0.6  # ä½ä¿¡å¿ƒåº¦ï¼Œæ˜é¡¯é™ä½
        else:
            # æ¥µä½ä¿¡å¿ƒåº¦ï¼šå‚¾å‘æ–¼å ±å‘Šç‚º"é¢¨éšªä¸æ˜"çš„ä¸­ä½åˆ†æ•¸å€åŸŸ
            return 0.3  # å¤§å¹…é™ä½ï¼Œé¿å…é«˜é¢¨éšªèª¤å ±

    def _apply_multi_class_weighting(self, probabilities: Dict, base_score: float) -> float:
        """
        æ‡‰ç”¨å¤šé¡åˆ¥æ©Ÿç‡åŠ æ¬Š
        è€ƒæ…®å…¶ä»–æ¼æ´é¡å‹çš„æ©Ÿç‡å°æœ€çµ‚åˆ†æ•¸çš„å½±éŸ¿
        """
        try:
            # è¨ˆç®—åŠ æ¬Šé¢¨éšªè²¢ç»
            weighted_contribution = 0.0
            
            for vuln_type, probability in probabilities.items():
                if vuln_type == "safe":
                    continue  # è·³éå®‰å…¨é¡åˆ¥
                
                # ç²å–è©²æ¼æ´é¡å‹çš„ä¸­ä½åˆ†æ•¸
                score_range = self.vulnerability_score_ranges.get(vuln_type, (0, 19))
                mid_score = (score_range[0] + score_range[1]) / 2
                
                # åŠ æ¬Šè²¢ç» = æ©Ÿç‡ Ã— è©²é¡å‹ä¸­ä½åˆ†æ•¸
                weighted_contribution += probability * mid_score
            
            # çµåˆåŸºç¤åˆ†æ•¸å’ŒåŠ æ¬Šè²¢ç» (70% åŸºç¤ + 30% åŠ æ¬Š)
            final_score = (base_score * 0.7) + (weighted_contribution * 0.3)
            
            return final_score
            
        except Exception as e:
            print(f"å¤šé¡åˆ¥åŠ æ¬Šè¨ˆç®—éŒ¯èª¤: {e}")
            return base_score

    def _convert_score_to_risk_level(self, risk_score: int) -> tuple:
        """
        å°‡100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸è½‰æ›ç‚ºé¢¨éšªç­‰ç´šå’Œå»ºè­°
        
        Args:
            risk_score: 0-100çš„é¢¨éšªåˆ†æ•¸
            
        Returns:
            tuple: (risk_level, recommendation, normalized_score)
        """
        # å°‡100åˆ†åˆ¶è½‰æ›ç‚º0-1ç¯„åœ (ç”¨æ–¼ç›¸å®¹æ€§)
        normalized_score = risk_score / 100.0
        
        if risk_score >= 70:
            return "HIGH", "ğŸš« æ‹’çµ• - æª¢æ¸¬åˆ°é«˜å®‰å…¨é¢¨éšª (MLåˆ†æ)", normalized_score
        elif risk_score >= 40:
            return "MEDIUM", "âš ï¸ è­¦å‘Š - è«‹è¬¹æ…è™•ç† (MLåˆ†æ)", normalized_score
        elif risk_score >= 20:
            return "LOW", "âœ… å¯æ¥å— - æª¢æ¸¬åˆ°ä½é¢¨éšª (MLåˆ†æ)", normalized_score
        else:
            return "SAFE", "âœ… æ‰¹å‡† - æœªæª¢æ¸¬åˆ°æ˜é¡¯é¢¨éšª (MLåˆ†æ)", normalized_score

    async def analyze_with_ml_integration(self, domain: str, permissions: List[str], 
                                        package_analyses: List[Dict], move_source_code: str = "") -> Dict:
        """
        çµåˆè¦å‰‡å¼•æ“å’Œæ©Ÿå™¨å­¸ç¿’çš„ç¶œåˆé¢¨éšªåˆ†æ
        """
        try:
            # åŸºç¤è¦å‰‡å¼•æ“åˆ†æ
            rule_based_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            
            # æ©Ÿå™¨å­¸ç¿’æ™ºèƒ½åˆç´„æ¼æ´åˆ†é¡
            ml_classification = None
            ml_risk_score = 0.0
            
            if move_source_code.strip():
                ml_classification = await self.classify_smart_contract_vulnerability(move_source_code)
                # ä½¿ç”¨æ–°çš„100åˆ†åˆ¶é¢¨éšªåˆ†æ•¸ (è½‰æ›ç‚º0-1ç¯„åœ)
                ml_risk_score = ml_classification.get('risk_score', 0) / 100.0
            
            # åˆä½µé¢¨éšªåˆ†æçµæœ
            rule_risk_score = rule_based_analysis['risk_breakdown']['final_score']
            
            # è¨ˆç®—ç¶œåˆé¢¨éšªåˆ†æ•¸
            if ml_classification and ml_classification.get('confidence', 0) > 0.3:
                # ML åˆ†é¡å¯ä¿¡åº¦é«˜æ™‚ï¼Œçµ¦äºˆæ›´é«˜æ¬Šé‡
                final_risk_score = (ml_risk_score * 0.6) + (rule_risk_score * 0.4)
                confidence_boost = 0.1
                # ä½¿ç”¨ ML 100 åˆ†åˆ¶é¢¨éšªåˆ†æ•¸åˆ¤æ–·ç­‰ç´š
                ml_score_100 = ml_classification.get('risk_score', 0)
                if ml_score_100 >= 70:
                    risk_level = "HIGH"
                    recommendation = "Reject - High security risk detected (ML+Rules)"
                elif ml_score_100 >= 40:
                    risk_level = "MEDIUM"
                    recommendation = "Warning - Please proceed with caution (ML+Rules)"
                else:
                    risk_level = "LOW"
                    recommendation = "Approve - Low risk detected (ML+Rules)"
            else:
                # ML åˆ†é¡ä¸å¯ç”¨æˆ–å¯ä¿¡åº¦ä½æ™‚ï¼Œä¸»è¦ä¾è³´è¦å‰‡å¼•æ“
                final_risk_score = (rule_risk_score * 0.8) + (ml_risk_score * 0.2)
                confidence_boost = 0.0
                # å¦‚æœæœ‰ ML åˆ†æ•¸ï¼Œä½¿ç”¨ ML 100 åˆ†åˆ¶åˆ¤æ–·ï¼ˆå³ä½¿ä¿¡å¿ƒåº¦ä½ï¼‰
                if ml_classification and ml_classification.get('risk_score', 0) > 0:
                    ml_score_100 = ml_classification.get('risk_score', 0)
                    if ml_score_100 >= 70:
                        risk_level = "HIGH"
                        recommendation = "Reject - High security risk detected (ML+Rules)"
                    elif ml_score_100 >= 40:
                        risk_level = "MEDIUM"
                        recommendation = "Warning - Please proceed with caution (ML+Rules)"
                    else:
                        risk_level = "LOW"
                        recommendation = "Approve - Low risk detected (ML+Rules)"
                else:
                    # ç´”è¦å‰‡å¼•æ“åˆ¤æ–·
                    if final_risk_score >= 0.7:
                        risk_level = "HIGH"
                        recommendation = "Reject - High security risk detected (Rules)"
                    elif final_risk_score >= 0.4:
                        risk_level = "MEDIUM"
                        recommendation = "Warning - Please proceed with caution (Rules)"
                    else:
                        risk_level = "LOW"
                        recommendation = "Approve - Low risk detected (Rules)"
            
            # åˆä½µé¢¨éšªåŸå› 
            all_reasons = rule_based_analysis['reasons'].copy()
            if ml_classification and ml_classification.get('classification') != 'safe':
                all_reasons.append(
                    f"ML detected smart contract vulnerability: {ml_classification['classification']} "
                    f"(confidence: {ml_classification.get('confidence', 0):.2f})"
                )
            
            return {
                "risk_level": risk_level,
                "confidence": round(final_risk_score + confidence_boost, 2),
                "reasons": all_reasons,
                "recommendation": recommendation,
                "risk_breakdown": {
                    **rule_based_analysis['risk_breakdown'],
                    "ml_vulnerability_score": round(ml_risk_score, 2),
                    "final_combined_score": round(final_risk_score, 2)
                },
                "ml_analysis": ml_classification,
                "details": {
                    **rule_based_analysis['details'],
                    "ml_enabled": bool(move_source_code.strip()),
                    "analysis_method": "hybrid_ml_rules",
                    "ml_risk_score_100": ml_classification.get('risk_score', 0) if ml_classification else 0,
                    "ml_probabilities": ml_classification.get('probabilities', {}) if ml_classification else {},
                    "processing_time": ml_classification.get('processing_time', 0) if ml_classification else 0
                }
            }
            
        except Exception as e:
            # å¦‚æœMLåˆ†æå¤±æ•—ï¼Œå›é€€åˆ°ç´”è¦å‰‡å¼•æ“
            rule_analysis = self.calculate_overall_risk(domain, permissions, package_analyses)
            rule_analysis['details']['ml_analysis_error'] = str(e)
            rule_analysis['details']['analysis_method'] = "rules_only_fallback"
            return rule_analysis
