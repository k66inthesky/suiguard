"""
ML Training Service
æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´å’Œæ¸¬è©¦æœå‹™
è² è²¬ LoRA å¾®èª¿æ¨¡å‹çš„è¨“ç·´ã€é©—è­‰å’Œè©•ä¼°
"""

import json
import time
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, PeftModel, AutoPeftModelForCausalLM
from datasets import Dataset
import numpy as np

class MLTrainingService:
    """ML è¨“ç·´æœå‹™é¡"""
    
    # å®šç¾©æ¨™æº–æ¼æ´é¡å‹
    VULNERABILITY_TYPES = {
        'Resource Leak': ['resource leak', 'è³‡æºæ´©æ¼', 'è³‡æºæ³„æ¼', 'è³‡æº'],
        'Arithmetic Overflow': ['arithmetic overflow', 'ç®—è¡“æº¢ä½', 'ç®—æœ¯æº¢å‡º', 'æº¢ä½', 'æº¢å‡º'],
        'Unchecked Return': ['unchecked return', 'æœªæª¢æŸ¥è¿”å›', 'æœªæ£€æŸ¥è¿”å›', 'è¿”å›å€¼', 'æœªæª¢æŸ¥', 'æœªæ£€æŸ¥'],
        'Cross-Module Pollution': ['cross-module pollution', 'è·¨æ¨¡çµ„æ±¡æŸ“', 'è·¨æ¨¡å—æ±¡æŸ“', 'æ¨¡çµ„æ±¡æŸ“', 'æ¨¡å—æ±¡æŸ“'],
        'Capability Leak': ['capability leak', 'æ¬Šé™æ´©æ¼', 'æƒé™æ³„æ¼', 'æ¬Šé™', 'æƒé™', 'capability', 'æ¬Šé™æª¢æŸ¥', 'æƒé™æ£€æŸ¥'],
        'æœªç™¼ç¾æ˜é¡¯æ¼æ´': ['æœªç™¼ç¾', 'æœªå‘ç°', 'å®‰å…¨', 'safe', 'ç„¡æ¼æ´', 'æ— æ¼æ´']
    }
    
    def __init__(self, 
                 base_model: str = "mistralai/Mistral-7B-Instruct-v0.2",
                 dataset_path: str = "ml/contract_bug_dataset.jsonl",
                 output_dir: str = "./lora_models"):
        """
        åˆå§‹åŒ– ML è¨“ç·´æœå‹™
        
        Args:
            base_model: åŸºç¤æ¨¡å‹åç¨±
            dataset_path: è¨“ç·´æ•¸æ“šé›†è·¯å¾‘
            output_dir: æ¨¡å‹è¼¸å‡ºç›®éŒ„
        """
        self.base_model = base_model
        self.dataset_path = dataset_path
        self.output_dir = output_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_dataset(self, path: Optional[str] = None) -> List[Dict]:
        """
        è¼‰å…¥è¨“ç·´æ•¸æ“šé›†
        
        Args:
            path: æ•¸æ“šé›†è·¯å¾‘ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é»˜èªè·¯å¾‘
            
        Returns:
            æ¨£æœ¬åˆ—è¡¨
        """
        path = path or self.dataset_path
        samples = []
        
        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                # è·³éè¨»é‡‹å’Œç©ºè¡Œ
                if line.startswith('//') or line.startswith('#') or not line:
                    continue
                try:
                    data = json.loads(line)
                    data['line_number'] = line_num
                    samples.append(data)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ è­¦å‘Šï¼šç¬¬ {line_num} è¡Œ JSON è§£æéŒ¯èª¤: {e}")
                    continue
                    
        print(f"âœ… è¼‰å…¥äº† {len(samples)} å€‹è¨“ç·´æ¨£æœ¬")
        return samples
    
    def prepare_training_data(self, samples: List[Dict]) -> Dict:
        """
        æº–å‚™è¨“ç·´æ•¸æ“šæ ¼å¼
        
        Args:
            samples: åŸå§‹æ¨£æœ¬åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„è¨“ç·´æ•¸æ“š
        """
        texts = []
        for sample in samples:
            # æ§‹å»ºå®Œæ•´çš„è¨“ç·´æ–‡æœ¬
            text = f"{sample['instruction']}\n{sample['input']}\n{sample['output']}"
            texts.append(text)
        
        return {"text": texts}
    
    def train_model(self,
                   num_epochs: int = 3,
                   batch_size: int = 2,
                   learning_rate: float = 3e-4,
                   lora_r: int = 8,
                   lora_alpha: int = 16,
                   lora_dropout: float = 0.05,
                   use_8bit: bool = True) -> Dict:
        """
        è¨“ç·´ LoRA å¾®èª¿æ¨¡å‹
        
        Args:
            num_epochs: è¨“ç·´è¼ªæ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¸ç¿’ç‡
            lora_r: LoRA ç§©
            lora_alpha: LoRA alpha åƒæ•¸
            lora_dropout: LoRA dropout
            use_8bit: æ˜¯å¦ä½¿ç”¨ 8-bit é‡åŒ–
            
        Returns:
            è¨“ç·´çµæœçµ±è¨ˆ
        """
        print("=" * 80)
        print("  ğŸš€ é–‹å§‹è¨“ç·´ LoRA æ¨¡å‹")
        print("=" * 80)
        print(f"åŸºç¤æ¨¡å‹: {self.base_model}")
        print(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        print(f"è¨“ç·´è¼ªæ•¸: {num_epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"å­¸ç¿’ç‡: {learning_rate}")
        print(f"LoRA ç§©: {lora_r}")
        print()
        
        # è¼‰å…¥æ•¸æ“š
        samples = self.load_dataset()
        train_data = self.prepare_training_data(samples)
        train_dataset = Dataset.from_dict(train_data)
        
        # è¼‰å…¥ tokenizer
        print("ğŸ“¥ è¼‰å…¥ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        tokenizer.pad_token = tokenizer.eos_token
        
        # Tokenize æ•¸æ“š
        def tokenize_function(examples):
            return tokenizer(examples["text"], truncation=True, max_length=512, padding=False)
        
        tokenized_dataset = train_dataset.map(
            tokenize_function, 
            batched=True, 
            remove_columns=["text"]
        )
        
        # è¼‰å…¥åŸºç¤æ¨¡å‹
        print("ğŸ“¥ è¼‰å…¥åŸºç¤æ¨¡å‹...")
        model_kwargs = {
            "device_map": "auto",
            "torch_dtype": torch.float16
        }
        
        if use_8bit:
            model_kwargs["load_in_8bit"] = True
            
        model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            **model_kwargs
        )
        
        # é…ç½® LoRA
        print("âš™ï¸ é…ç½® LoRA...")
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["q_proj", "v_proj"],
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # è¨“ç·´åƒæ•¸
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=1,
            learning_rate=learning_rate,
            logging_steps=10,
            save_strategy="epoch",
            save_total_limit=2,
            fp16=True,
            optim="adamw_torch",
            report_to="none"
        )
        
        # æ•¸æ“šæ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
        
        # è¨“ç·´å™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # é–‹å§‹è¨“ç·´
        print("ğŸ”¥ é–‹å§‹è¨“ç·´...")
        start_time = time.time()
        train_result = trainer.train()
        train_time = time.time() - start_time
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        model.save_pretrained(self.output_dir)
        tokenizer.save_pretrained(self.output_dir)
        
        # è¨“ç·´çµ±è¨ˆ
        training_stats = {
            "train_time": round(train_time, 2),
            "train_samples": len(samples),
            "num_epochs": num_epochs,
            "final_loss": float(train_result.training_loss),
            "model_path": self.output_dir,
            "timestamp": datetime.now().isoformat()
        }
        
        # ä¿å­˜è¨“ç·´çµ±è¨ˆ
        stats_path = os.path.join(self.output_dir, "training_stats.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è¨“ç·´å®Œæˆï¼")
        print(f"   æ™‚é–“: {train_time:.2f} ç§’ ({train_time/60:.2f} åˆ†é˜)")
        print(f"   æœ€çµ‚ Loss: {train_result.training_loss:.4f}")
        print(f"   æ¨¡å‹ä¿å­˜åˆ°: {self.output_dir}")
        print("=" * 80)
        
        return training_stats
    
    def test_model(self, 
                  model_path: Optional[str] = None,
                  test_samples: Optional[List[Dict]] = None) -> Dict:
        """
        æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾‘ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é»˜èªè¼¸å‡ºç›®éŒ„
            test_samples: æ¸¬è©¦æ¨£æœ¬ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨å®Œæ•´æ•¸æ“šé›†
            
        Returns:
            æ¸¬è©¦çµæœçµ±è¨ˆ
        """
        model_path = model_path or self.output_dir
        
        print("=" * 80)
        print("  ğŸ§ª æ¸¬è©¦æ¨¡å‹")
        print("=" * 80)
        print(f"æ¨¡å‹è·¯å¾‘: {model_path}")
        
        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        if test_samples is None:
            test_samples = self.load_dataset()
        
        print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_samples)}")
        
        # è¼‰å…¥æ¨¡å‹
        print("\nğŸ“¥ è¼‰å…¥æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        model = AutoPeftModelForCausalLM.from_pretrained(
            model_path,
            device_map={"": self.device},
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        )
        model.eval()
        
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\n")
        
        # æ¸¬è©¦
        results = []
        correct = 0
        total_time = 0
        total_similarity = 0
        
        # æŒ‰æ¼æ´é¡å‹çµ±è¨ˆ
        vuln_stats = {}
        
        print("=" * 80)
        print("  é–‹å§‹æ¸¬è©¦...")
        print("=" * 80)
        print()
        
        for i, sample in enumerate(test_samples, 1):
            instruction = sample['instruction']
            input_text = sample['input']
            expected = sample['output']
            
            # æ¨ç†
            output, inference_time = self._inference(
                model, tokenizer, instruction, input_text
            )
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarity = self._calculate_similarity(expected, output)
            is_correct = similarity >= 70
            
            if is_correct:
                correct += 1
            
            total_time += inference_time
            total_similarity += similarity
            
            # æå–æ¼æ´é¡å‹
            vuln_type = self._extract_vulnerability_type(expected)
            
            if vuln_type not in vuln_stats:
                vuln_stats[vuln_type] = {'total': 0, 'correct': 0}
            vuln_stats[vuln_type]['total'] += 1
            if is_correct:
                vuln_stats[vuln_type]['correct'] += 1
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"{status} æ¨£æœ¬ {i:2d}/{len(test_samples)} | ç›¸ä¼¼åº¦: {similarity:3d}% | æ™‚é–“: {inference_time:.2f}s | {vuln_type}")
            
            results.append({
                'sample_id': i,
                'instruction': instruction,
                'input': input_text,
                'expected': expected,
                'actual': output,
                'similarity': similarity,
                'correct': is_correct,
                'inference_time': inference_time,
                'vulnerability_type': vuln_type
            })
        
        # è¨ˆç®—çµ±è¨ˆ
        accuracy = (correct / len(test_samples)) * 100
        avg_similarity = total_similarity / len(test_samples)
        avg_time = total_time / len(test_samples)
        
        # é¡¯ç¤ºçµæœ
        print()
        print("=" * 80)
        print("  ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
        print("=" * 80)
        print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_samples)}")
        print(f"âœ… æ­£ç¢ºæ•¸é‡: {correct}")
        print(f"âŒ éŒ¯èª¤æ•¸é‡: {len(test_samples) - correct}")
        print(f"æº–ç¢ºç‡: {accuracy:.1f}%")
        print(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.1f}%")
        print(f"å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.2f} ç§’")
        print(f"ç¸½æ¨ç†æ™‚é–“: {total_time:.2f} ç§’")
        print()
        print("=" * 80)
        print("  ğŸ“‹ æŒ‰æ¼æ´é¡å‹åˆ†æ")
        print("=" * 80)
        
        for vtype in sorted(vuln_stats.keys()):
            stat = vuln_stats[vtype]
            acc = (stat['correct'] / stat['total'] * 100) if stat['total'] > 0 else 0
            print(f"{vtype}: {stat['correct']}/{stat['total']} ({acc:.1f}%)")
        
        print("=" * 80)
        
        # ä¿å­˜çµæœ
        test_results = {
            'test_time': datetime.now().isoformat(),
            'model_path': model_path,
            'summary': {
                'total_samples': len(test_samples),
                'correct_count': correct,
                'accuracy': round(accuracy, 2),
                'avg_similarity': round(avg_similarity, 2),
                'avg_time': round(avg_time, 4),
                'total_time': round(total_time, 2)
            },
            'vulnerability_stats': vuln_stats,
            'results': results
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        results_path = os.path.join(model_path, "test_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è©³ç´°çµæœå·²ä¿å­˜åˆ°: {results_path}")
        print("=" * 80)
        
        return test_results
    
    def _inference(self, model, tokenizer, instruction: str, input_text: str) -> Tuple[str, float]:
        """åŸ·è¡Œæ¨ç†"""
        prompt = f"{instruction}\n{input_text}"
        
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        if self.device == "cuda":
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )
        inference_time = time.time() - start_time
        
        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if full_output.startswith(prompt):
            output = full_output[len(prompt):].strip()
        else:
            output = full_output
        
        return output, inference_time
    
    def _calculate_similarity(self, expected: str, actual: str) -> int:
        """è¨ˆç®—ç›¸ä¼¼åº¦"""
        expected_lower = expected.lower()
        actual_lower = actual.lower()
        
        score = 0
        
        for vuln_type, keywords in self.VULNERABILITY_TYPES.items():
            if any(keyword.lower() in expected_lower for keyword in keywords):
                if any(keyword.lower() in actual_lower for keyword in keywords):
                    score = 70
                    
                    # é¡å¤–é—œéµè©åŠ åˆ†
                    if vuln_type == 'Resource Leak':
                        bonus_keywords = ['object', 'ç‰©ä»¶', 'transfer', 'ç§»äº¤', 'destroy', 'éŠ·æ¯€', 'è³‡æº']
                    elif vuln_type == 'Arithmetic Overflow':
                        bonus_keywords = ['overflow', 'underflow', 'æº¢ä½', 'ä¸‹æº¢', 'checked', 'ç®—è¡“']
                    elif vuln_type == 'Unchecked Return':
                        bonus_keywords = ['return', 'è¿”å›', 'check', 'æª¢æŸ¥', 'error', 'æœªæª¢æŸ¥']
                    elif vuln_type == 'Cross-Module Pollution':
                        bonus_keywords = ['module', 'æ¨¡çµ„', 'global', 'å…¨å±€', 'shared', 'è·¨']
                    elif vuln_type == 'Capability Leak':
                        bonus_keywords = ['capability', 'æ¬Šé™', 'permission', 'admin', 'æª¢æŸ¥']
                    else:
                        bonus_keywords = ['safe', 'å®‰å…¨', 'no', 'ç„¡', 'æœªç™¼ç¾']
                    
                    bonus = sum(10 for kw in bonus_keywords if kw.lower() in actual_lower)
                    score = min(100, score + bonus)
                    break
        
        return score
    
    def _extract_vulnerability_type(self, output: str) -> str:
        """å¾è¼¸å‡ºä¸­æå–æ¼æ´é¡å‹"""
        if 'æœªç™¼ç¾' in output or 'å®‰å…¨' in output:
            return 'æœªç™¼ç¾æ˜é¡¯æ¼æ´'
        elif 'æ¼æ´é¡å‹ï¼š' in output:
            return output.split('æ¼æ´é¡å‹ï¼š')[1].split('ï¼Œ')[0]
        else:
            # å˜—è©¦å¾é—œéµè©åŒ¹é…
            for vtype, keywords in self.VULNERABILITY_TYPES.items():
                if any(kw.lower() in output.lower() for kw in keywords):
                    return vtype
            return 'æœªçŸ¥'
    
    def cross_validate(self, n_folds: int = 3) -> Dict:
        """
        åŸ·è¡Œ K-fold äº¤å‰é©—è­‰
        
        Args:
            n_folds: æŠ˜æ•¸
            
        Returns:
            äº¤å‰é©—è­‰çµæœ
        """
        print("=" * 80)
        print(f"  ğŸ¯ {n_folds}-Fold äº¤å‰é©—è­‰")
        print("=" * 80)
        
        # è¼‰å…¥å®Œæ•´æ•¸æ“šé›†
        all_samples = self.load_dataset()
        n_samples = len(all_samples)
        fold_size = n_samples // n_folds
        
        fold_results = []
        
        for fold in range(n_folds):
            print(f"\n{'='*80}")
            print(f"  ğŸ“ Fold {fold + 1}/{n_folds}")
            print(f"{'='*80}")
            
            # åˆ†å‰²æ•¸æ“š
            test_start = fold * fold_size
            test_end = test_start + fold_size if fold < n_folds - 1 else n_samples
            
            test_samples = all_samples[test_start:test_end]
            train_samples = all_samples[:test_start] + all_samples[test_end:]
            
            print(f"è¨“ç·´é›†: {len(train_samples)} æ¨£æœ¬")
            print(f"æ¸¬è©¦é›†: {len(test_samples)} æ¨£æœ¬")
            
            # è¨“ç·´ (ä½¿ç”¨è‡¨æ™‚ç›®éŒ„)
            temp_output = f"{self.output_dir}_fold_{fold}"
            original_output = self.output_dir
            self.output_dir = temp_output
            
            # æš«æ™‚ä¿å­˜è¨“ç·´æ¨£æœ¬
            temp_dataset = f"temp_fold_{fold}.jsonl"
            with open(temp_dataset, 'w', encoding='utf-8') as f:
                for sample in train_samples:
                    json.dump(sample, f, ensure_ascii=False)
                    f.write('\n')
            
            original_dataset = self.dataset_path
            self.dataset_path = temp_dataset
            
            # è¨“ç·´
            train_stats = self.train_model(num_epochs=1, batch_size=2)
            
            # æ¸¬è©¦
            test_results = self.test_model(test_samples=test_samples)
            
            # æ¢å¾©è¨­ç½®
            self.output_dir = original_output
            self.dataset_path = original_dataset
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            import shutil
            if os.path.exists(temp_output):
                shutil.rmtree(temp_output)
            if os.path.exists(temp_dataset):
                os.remove(temp_dataset)
            
            fold_results.append({
                'fold': fold + 1,
                'train_stats': train_stats,
                'test_results': test_results['summary']
            })
        
        # è¨ˆç®—å¹³å‡çµæœ
        avg_accuracy = np.mean([r['test_results']['accuracy'] for r in fold_results])
        std_accuracy = np.std([r['test_results']['accuracy'] for r in fold_results])
        
        cv_results = {
            'n_folds': n_folds,
            'total_samples': n_samples,
            'avg_accuracy': round(avg_accuracy, 2),
            'std_accuracy': round(std_accuracy, 2),
            'fold_results': fold_results,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"\n{'='*80}")
        print(f"  ğŸ¯ äº¤å‰é©—è­‰çµæœ")
        print(f"{'='*80}")
        print(f"å¹³å‡æº–ç¢ºç‡: {avg_accuracy:.1f}% Â± {std_accuracy:.1f}%")
        print(f"{'='*80}")
        
        # ä¿å­˜çµæœ
        cv_path = os.path.join(self.output_dir, "cross_validation_results.json")
        with open(cv_path, 'w', encoding='utf-8') as f:
            json.dump(cv_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… äº¤å‰é©—è­‰çµæœå·²ä¿å­˜åˆ°: {cv_path}")
        
        return cv_results


# ä¾¿æ·å‡½æ•¸
def train_vulnerability_detection_model(
    base_model: str = "mistralai/Mistral-7B-Instruct-v0.2",
    dataset_path: str = "ml/contract_bug_dataset.jsonl",
    output_dir: str = "./lora_models",
    num_epochs: int = 3
) -> Dict:
    """
    è¨“ç·´æ¼æ´æª¢æ¸¬æ¨¡å‹çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        base_model: åŸºç¤æ¨¡å‹åç¨±
        dataset_path: æ•¸æ“šé›†è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„
        num_epochs: è¨“ç·´è¼ªæ•¸
        
    Returns:
        è¨“ç·´çµ±è¨ˆçµæœ
    """
    service = MLTrainingService(base_model, dataset_path, output_dir)
    return service.train_model(num_epochs=num_epochs)


def test_vulnerability_detection_model(
    model_path: str = "./lora_models",
    dataset_path: str = "ml/contract_bug_dataset.jsonl"
) -> Dict:
    """
    æ¸¬è©¦æ¼æ´æª¢æ¸¬æ¨¡å‹çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        dataset_path: æ¸¬è©¦æ•¸æ“šé›†è·¯å¾‘
        
    Returns:
        æ¸¬è©¦çµæœ
    """
    service = MLTrainingService(dataset_path=dataset_path, output_dir=model_path)
    return service.test_model()
